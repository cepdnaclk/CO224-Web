\section{Lecture 14: Introduction to Memory Systems and Cache Memory}

\emph{By Dr. Isuru Nawinne}

\subsection{Introduction}

This lecture marks a crucial transition from CPU-centric topics to memory systems, introducing cache memory as the elegant solution to the fundamental processor-memory speed gap. We begin with historical context, tracing how stored-program concept revolutionized computing, then explore the memory hierarchy that creates the illusion of large, fast memory through careful exploitation of temporal and spatial locality. The direct-mapped cache organization receives detailed treatment, establishing foundational concepts of blocks, tags, indices, and valid bits that underpin all cache designs. Understanding cache memory proves as essential as understanding processor architecture, as memory system performance often determines overall computer system speed in practice.

\subsection{Lecture Introduction and Historical Context}

\subsubsection{Lecture Transition}

\textbf{Previous Topics:}

\begin{itemize}
\item CPU datapath and control (ARM, MIPS, pipelining)

\textbf{New Focus:}

\begin{itemize}
\item Memory systems (equally important as CPU)

\textbf{Motivation:}

\begin{itemize}
\item Memory plays as significant a role as CPU in modern computer architecture

\subsubsection{Historical Background}

\paragraph{Early Computing Machines (1940s)}

\textbf{Examples:}

\begin{itemize}
\item ENIAC (University of Pennsylvania)
\item Harvard Mark I (ASTC)

\textbf{Characteristics:}

\begin{itemize}
\item Filled entire rooms
\item Built using vacuum tubes and electrical circuitry
\item Developed for war efforts (World War II)
\item Used for artillery planning, nuclear weapon calculations
\item No concept of software or memory as we know today

\textbf{Programming Method:}

\begin{itemize}
\item Rewiring the entire machine for each algorithm
\item Engineers spent days/weeks reconfiguring machines
\item No stored program concept

\subsubsection{Key Historical Figures}

\paragraph{Alan Turing (1936)}

\begin{itemize}
\item British mathematician, brilliant mind
\item First conceived the stored program computer concept
\item Designed the Universal Turing Machine (hypothetical machine)
\item First notion of memory, programs stored in computers, data read/write operations
\item Later involved in World War II cryptography (Enigma Machine, "The Imitation Game")

\paragraph{John von Neumann (1940s)}

\begin{itemize}
\item Hungarian mathematician, regarded as "last of the brilliant mathematicians"
\item Prodigy: Solving calculus problems by age 8
\item Contributed across many fields
\item Got involved with EDVAC computer project
\item Implemented stored program concept based on Turing's ideas

\subsubsection{First Stored Program Computers}

\paragraph{EDVAC (1948)}

\begin{itemize}
\item Commissioned by U.S. Army
\item John von Neumann involved as consultant
\item Memory: Initially 1044 words, upgraded to 1024 words (power of 2)
\item First machine with stored program concept
\item Memory stored program electrically (not in wiring)
\item Engineers created the first "memory" device
\item First test programs: Nuclear weapon detonation calculations, hydrogen bomb calculations

\paragraph{Von Neumann Architecture}

\textbf{Key Concept:}

\begin{itemize}
\item Data AND instructions both in SAME memory
\item Access data and programs through SAME connection pathways
\item Unified memory for instructions and data
\item This concept became foundation of modern computers

\paragraph{EDSAC (Cambridge University)}

\begin{itemize}
\item Built about a year after EDVAC
\item First machine fully implementing Von Neumann architecture
\item Memory: 512 words of 18 bits each
\item Also built for war effort

\paragraph{Harvard Architecture (Contrasted)}

\begin{itemize}
\item Separate storages for instructions and data
\item Separate connections to instruction memory and data memory
\item Used in MIPS datapath design (separate instruction memory and data memory)

\textbf{Modern Computers:}

\begin{itemize}
\item Use a MIX of both Von Neumann and Harvard architectures
\item Features from both types incorporated

\subsection{Memory Technologies: Types and Characteristics}

\subsubsection{Commonly Used Memory Technologies Today}

\begin{itemize}
\item SRAM (Static RAM)
\item DRAM (Dynamic RAM)
\item Flash Memory
\item Magnetic Disk
\item Magnetic Tape

\subsubsection{SRAM (Static RAM)}

| Property            | Value/Description                          |
| ------------------- | ------------------------------------------ |
| \textbf{Technology}      | Built using flip-flops                     |
| \textbf{Volatility}      | Volatile (loses content when power lost)   |
| \textbf{Access Time}     | Less than 1 nanosecond (< 1 ns)            |
| \textbf{Clock Frequency} | More than 1 GHz                            |
| \textbf{Cycle Time}      | Less than 1 nanosecond (< 1 ns)            |
| \textbf{Capacity}        | Kilobytes to Megabytes range               |
| \textbf{Cost}            | ~$2000 per gigabyte (VERY EXPENSIVE)       |
| \textbf{Speed}           | Extremely fast                             |
| \textbf{Usage}           | Cache memories (small amounts due to cost) |

\textbf{Note on Cycle Time:}

\begin{itemize}
\item Cycle time = minimum time between two consecutive memory accesses
\item Access time ≈ Cycle time for SRAM

\subsubsection{DRAM (Dynamic RAM)}

| Property        | Value/Description                              |
| --------------- | ---------------------------------------------- |
| \textbf{Technology}  | Transistors + Capacitors                       |
| \textbf{Volatility}  | Volatile (requires power AND periodic refresh) |
| \textbf{Access Time} | ~25 nanoseconds (50 ns in some contexts)       |
| \textbf{Cycle Time}  | ~50 nanoseconds (double the access time)       |
| \textbf{Capacity}    | Gigabytes (8 GB, 16 GB, or more)               |
| \textbf{Cost}        | ~$10 per gigabyte                              |
| \textbf{Usage}       | Main memory in computers                       |

\textbf{Key Characteristics:}

\begin{itemize}
\item Capacitor charge must be maintained
\item "Destructive read": Reading loses the charge, requires rewrite/refresh
\item Longer cycle time due to refresh requirement
\item After reading, must rewrite data to same cell
\item Significantly slower than SRAM (25-50 ns vs < 1 ns)

\subsubsection{Flash Memory}

| Property        | Value/Description                                     |
| --------------- | ----------------------------------------------------- |
| \textbf{Technology}  | NAND MOSFET (NAND gate with two gates)                |
| \textbf{Volatility}  | Non-volatile (retains data without power)             |
| \textbf{Access Time} | ~70 nanoseconds                                       |
| \textbf{Cycle Time}  | ~70 nanoseconds                                       |
| \textbf{Capacity}    | Gigabytes range                                       |
| \textbf{Cost}        | Less than $1 per gigabyte                             |
| \textbf{Usage}       | Secondary storage (SSDs - Solid State Devices/Drives) |

\textbf{Limitation:}

\begin{itemize}
\item Limited read/write cycles
\item After several thousand cycles, memory cells may degrade
\item Integrity decreases, capacity effectively decreases
\item Slightly slower than DRAM, but non-volatile

\subsubsection{Magnetic Disk}

| Property        | Value/Description                                          |
| --------------- | ---------------------------------------------------------- |
| \textbf{Technology}  | Magnetic (mechanical device)                               |
| \textbf{Access Time} | 5 to 10 milliseconds (MUCH slower than electronic memory!) |
| \textbf{Cycle Time}  | Similar to access time (~5-10 ms)                          |
| \textbf{Capacity}    | Several terabytes                                          |
| \textbf{Cost}        | Fraction of a dollar per gigabyte (very cheap)             |

\textbf{Usage:}

\begin{itemize}
\item Previously: Main secondary storage
\item Currently: Being replaced by flash/SSDs for secondary storage
\item Now used primarily for tertiary storage, backups
\item Good for long-term data retention, low cost
\item Slowness acceptable for infrequent backup operations

\textbf{Note:} Average numbers; varies by data location on disk. Mechanical: spinning platters, moving read/write heads.

\subsection{The Memory Performance Problem}

\subsubsection{The CPU-Memory Speed Gap}

\textbf{CPU Clock Cycle:}

\begin{itemize}
\item Modern CPUs: > 1 GHz clock frequency
\item Clock cycle: < 1 nanosecond (1 ns corresponds to 1 GHz)

\textbf{Main Memory (DRAM):}

\begin{itemize}
\item Cycle time: ~50 nanoseconds
\item Time between starts of two consecutive memory accesses: 50 ns

\subsubsection{The Problem}

\textbf{Speed Discrepancy:}

\begin{itemize}
\item CPU cycle: < 1 ns
\item Memory cycle: 50 ns
\item \textbf{Memory is 50$\times$ SLOWER than CPU!}

\subsubsection{Impact on Pipelining}

\textbf{The Challenge:}

\begin{itemize}
\item In MIPS pipeline, MEM stage must finish in ONE clock cycle
\item Every pipeline stage must take same time
\item How can MEM stage complete in 1 ns when memory takes 50 ns?
\item Pipeline performance would be severely degraded

\textbf{The Contradiction:}

\begin{itemize}
\item CPU expects 1 ns memory access
\item Actual DRAM takes 50 ns
\item "Something is not right" - how can this work?

\subsection{Memory Hierarchy Concept}

\subsubsection{The Solution: Memory Hierarchy}

\textbf{Core Idea:}

\begin{itemize}
\item Trick the CPU into thinking memory is BOTH fast AND large
\item Desired characteristics:
\item Fast access times (like SRAM: < 1 ns)
\item Large capacity (like Disk: terabytes)
\item These characteristics don't exist in single technology
\item Solution: Implement memory as a HIERARCHY

\subsubsection{Memory Hierarchy Structure}

Level 1 (Top): SRAM (Cache)
\begin{itemize}
\item Smallest capacity
\item Fastest speed
\item Closest to CPU physically

Level 2: DRAM (Main Memory)
\begin{itemize}
\item Medium capacity
\item Medium speed

Level 3 (Bottom): Disk
\begin{itemize}
\item Largest capacity
\item Slowest speed

\subsubsection{Key Principles}

\paragraph{1. CPU Access Restriction}

\begin{itemize}
\item CPU can ONLY access top level (SRAM cache)
\item CPU thinks cache is the actual memory
\item CPU cannot directly access DRAM or Disk

\paragraph{2. CPU's Perception}

\begin{itemize}
\item Experiences the SPEED of SRAM
\item Feels the CAPACITY of DRAM and Disk combined
\item Illusion: Memory is as fast as SRAM AND as big as lowest level

\paragraph{3. Data Organization}

\begin{itemize}
\item Upper levels contain SUBSET of data from lower levels
\item SRAM (few MB) contains subset of DRAM (several GB)
\item DRAM contains subset of Disk (several TB)
\item At any given time, each level holds only a fraction of lower level's data

\paragraph{4. Hierarchy Characteristics}

\begin{itemize}
\item Devices up the hierarchy: Smaller and faster
\item Devices down the hierarchy: Larger but slower

\subsubsection{The Challenge}

\textbf{What if CPU asks for data NOT in the cache (top level)?}

\begin{itemize}
\item Need mechanism to copy data from lower levels
\item This leads to the concepts of hits, misses, and cache management

\subsection{Analogy: Music Library}

\subsubsection{Understanding Memory Hierarchy Through Music}

\paragraph{Three-Level Music System}

\textbf{1. Mobile Phone (analogous to SRAM/Cache):}

\begin{itemize}
\item Carries a subset of your favorite songs
\item Always with you
\item Listen to music directly from phone
\item Limited storage (like cache has limited capacity)

\textbf{2. Computer Hard Disk (analogous to DRAM/Main Memory):}

\begin{itemize}
\item Main music collection stored here
\item Larger collection than phone
\item Not always accessible (not in pocket)
\item Copy songs from here to phone when needed

\textbf{3. Internet (analogous to Disk/Mass Storage):}

\begin{itemize}
\item All songs available (massive storage)
\item Download/buy songs from here
\item Copy to computer, then to phone

\subsubsection{Usage Scenarios}

\paragraph{Scenario 1 (Hit)}

\begin{itemize}
\item Want to listen to a song
\item Song is already on phone
\item Just play it directly
\item Similar to cache hit: Data already in cache

\paragraph{Scenario 2 (Miss to Level 2)}

\begin{itemize}
\item Want to listen to a song
\item Song NOT on phone
\item Must go to computer and copy to phone
\item Then listen on phone
\item Similar to cache miss: Must fetch from main memory

\paragraph{Scenario 3 (Miss to Level 3)}

\begin{itemize}
\item Want to listen to a song
\item Song NOT on phone AND NOT on computer
\item Download from internet to computer
\item Copy to phone
\item Then listen
\item Similar to cache miss to disk: Must fetch from lowest level

\subsubsection{Key Parallels}

\begin{itemize}
\item Always listen from phone (CPU always accesses cache)
\item Main collection in computer (main memory holds primary data)
\item All data available on internet (disk holds everything)
\item Copy operations when data not available at higher levels

\subsection{Memory Hierarchy Terminology}

\subsubsection{Essential Terms for Memory Access}

\paragraph{HIT}

\textbf{Definition:} Requested data IS available at the accessed level

\begin{itemize}
\item CPU requests data $\rightarrow$ Data found in cache
\item Like wanting to listen to song already on your phone
\item Can be served immediately from that level

\paragraph{MISS}

\textbf{Definition:} Requested data is NOT available at the accessed level

\begin{itemize}
\item CPU requests data $\rightarrow$ Data NOT found in cache
\item Like wanting to listen to song not on your phone
\item Must fetch from lower level in hierarchy

\paragraph{HIT RATE}

\textbf{Definition:} Ratio/percentage of accesses that result in hits

\textbf{Formula:}

Hit Rate = (Number of Hits) / (Total Accesses)

\textbf{Example:} 100 accesses, 90 hits $\rightarrow$ Hit Rate = 90% or 0.9

Indicates how often data is found at the accessed level. Higher hit rate = better performance.

\paragraph{MISS RATE}

\textbf{Definition:} Ratio/percentage of accesses that result in misses

\textbf{Formula:}

Miss Rate = (Number of Misses) / (Total Accesses)
Miss Rate = 1 - Hit Rate

\textbf{Example:} 100 accesses, 10 misses $\rightarrow$ Miss Rate = 10% or 0.1

Lower miss rate = better performance.

\paragraph{HIT LATENCY}

\textbf{Definition:} Time taken to determine if access is a hit AND serve the data

\begin{itemize}
\item Time to check if data is in cache and deliver it to CPU
\item For SRAM cache: < 1 nanosecond

\textbf{Components:}

\begin{itemize}
\item Time to search cache
\item Time to verify data presence
\item Time to extract and send data to CPU

\paragraph{MISS PENALTY}

\textbf{Definition:} EXTRA time required when access is a miss

\textbf{Process:}

\begin{enumerate}
\item Determine it's a miss (hit latency spent)
\item Go to next level (DRAM)
\item Find the data
\item Copy to cache
\item Put in appropriate place
\item Deliver to CPU

\textbf{Key Points:}

\begin{itemize}
\item Total time on miss = Hit Latency + Miss Penalty
\item Miss penalty for DRAM access can be 100$\times$ hit latency
\item Very expensive in terms of time!

\subsection{Performance Impact and Requirements}

\subsubsection{Average Memory Access Time}

\textbf{Formula:}

Average Access Time = Hit Latency + (Miss Rate $\times$ Miss Penalty)

\textbf{Explanation:}

\begin{itemize}
\item ALL accesses consume hit latency (must check cache)
\item Only misses consume additional miss penalty
\item Miss Rate determines portion of accesses incurring penalty

\subsubsection{Example Analysis}

\textbf{Given:}

\begin{itemize}
\item Hit Latency (SRAM): < 1 nanosecond
\item Miss Penalty (DRAM access): ~100 nanoseconds (100$\times$ slower)
\item CPU clock cycle: < 1 nanosecond

\paragraph{For Pipeline to Work}

\begin{itemize}
\item MEM stage must complete in 1 clock cycle
\item Memory access must complete in < 1 ns most of the time

\paragraph{Required Hit Rate Calculation}

\textbf{If Hit Rate = 99.9\% (Miss Rate = 0.1\%):}

Average Time = 1 ns + (0.001 $\times$ 100 ns)
             = 1 ns + 0.1 ns
             = 1.1 ns

Still close to 1 clock cycle!

\textbf{If Hit Rate = 90\% (Miss Rate = 10\%):}

Average Time = 1 ns + (0.10 $\times$ 100 ns)
             = 1 ns + 10 ns
             = 11 ns

Unacceptable! 11$\times$ slower than CPU clock!

\subsubsection{Critical Requirement}

\begin{itemize}
\item Need VERY HIGH hit rate at cache level
\item Not just high, but VERY, VERY high
\item Target: \textbf{99.9\% or better}
\item Only 0.1% of accesses should go to memory

\subsubsection{Performance Implications}

\paragraph{With 99.9\% Hit Rate}

\begin{itemize}
\item 99.9% of time: CPU works fine, memory appears fast
\item 0.1% of time: CPU must STALL, wait for data from DRAM
\item Stall is unavoidable for misses
\item Overall: CPU maintains illusion of fast, large memory

\paragraph{With Lower Hit Rate}

\begin{itemize}
\item More frequent stalls
\item Pipeline performance degrades significantly
\item Average memory access time increases
\item CPU slows down dramatically

\textbf{Conclusion:}

\begin{itemize}
\item Must ensure VERY high hit rate at SRAM level
\item Memory hierarchy only works if locality principles hold
\item Like having most songs you want to listen to already on phone
\item Don't want to copy from computer frequently (time-consuming)

\subsection{Principles of Locality}

\subsubsection{Foundation for Memory Hierarchy Success}

\textbf{Nature of Computer Programs:}

\begin{itemize}
\item Programs access only SMALL portion of entire address space at any given time
\item Address space: Entire memory range (address 0 to maximum address)
\item At any time window, program uses only small fraction of total data
\item True by nature of how programs are written, compiled, and executed
\item True for instruction sets like ARM, MIPS

\subsubsection{Temporal Locality (Locality in Time)}

\paragraph{Definition}

\textbf{"Recently accessed data are likely to be accessed again soon"}

\textbf{Explanation:}

\begin{itemize}
\item If you access memory address A at time T
\item High probability of accessing address A again at time T+ΔT (soon after)
\item Same data accessed multiple times in short time window
\item "Locality in time" - data clustered temporally

\paragraph{Common Examples in Programs}

\textbf{a) Loop Index Variables:}

\begin{lstlisting}[language=c]
for (int i = 0; i < 100; i++) {
    // i is accessed every iteration
    // Same memory location for 'i' accessed repeatedly
}
\end{verbatim}

\textbf{b) Loop-Invariant Data:}

\begin{lstlisting}[language=c]
for (int i = 0; i < n; i++) {
    result = result + array[i] * constant;
    // 'result' and 'constant' accessed every iteration
}
\end{verbatim}

\textbf{c) Function/Procedure Calls:}

\begin{itemize}
\item Local variables accessed multiple times during function execution
\item Same stack frame locations accessed repeatedly

\textbf{d) Instructions:}

\begin{itemize}
\item Loop body instructions executed many times
\item Same instruction addresses accessed repeatedly

\paragraph{Music Analogy}

\begin{itemize}
\item If you listen to a song, you're likely to listen to it again soon
\item Sometimes listen to same song 10 times in a row
\item Want to replay favorite songs

\paragraph{Degree of Temporal Locality}

\begin{itemize}
\item Varies from program to program
\item But present in nearly ALL programs
\item Stronger in some (tight loops) than others

\subsubsection{Spatial Locality (Locality in Space)}

\paragraph{Definition}

\textbf{"Data located close to recently accessed data are likely to be accessed soon"}

\textbf{Explanation:}

\begin{itemize}
\item If you access memory address A at time T
\item High probability of accessing addresses A+1, A+2, A+3, ... soon after
\item Sequential or nearby addresses accessed together
\item "Locality in space" - data clustered spatially in memory

\paragraph{Common Examples in Programs}

\textbf{a) Array Traversal:}

\begin{lstlisting}[language=c]
for (int i = 0; i < 100; i++) {
    sum += array[i];
    // Access array[0], then array[1], then array[2], ...
    // Sequential memory addresses
}
\end{verbatim}

\textbf{b) Sequential Instruction Execution:}

\begin{itemize}
\item Instructions stored sequentially in memory
\item PC increments: fetch instruction at PC, then PC+4, then PC+8, ...
\item Except for branches, mostly sequential

\textbf{c) Data Structures:}

\begin{lstlisting}[language=c]
struct Student {
    int id;
    char name[50];
    float gpa;
};
Student s;
// Accessing s.id, then s.name, then s.gpa
// Nearby memory locations
\end{verbatim}

\textbf{d) String Processing:}

\begin{lstlisting}[language=c]
char str[] = "Hello";
for (int i = 0; str[i] != '\0'; i++) {
    // Access str[0], str[1], str[2], ...
    // Consecutive bytes in memory
}
\end{verbatim}

\paragraph{Music Analogy}

\begin{itemize}
\item If you listen to song by artist X, likely to listen to another song by artist X
\item If you listen to song from album Y, likely to listen to next song in album Y
\item Related/nearby songs accessed together

\paragraph{Degree of Spatial Locality}

\begin{itemize}
\item Varies by data access patterns
\item Strong in array-based algorithms
\item Present in most structured programs

\subsubsection{Universal Applicability}

\begin{itemize}
\item Both principles hold true for NEARLY ALL programs
\item Degree varies, but principles universally applicable
\item Foundation assumptions for cache design

\subsection{Cache Memory Concept and Block-Based Operation}

\subsubsection{Cache Memory Overview}

\textbf{Purpose:}

\begin{itemize}
\item Memory device at top level of hierarchy
\item Based on two principles of locality
\item Decides what data to keep based on locality principles

\subsubsection{Data Organization: BLOCKS}

\textbf{Key Concepts:}

\begin{itemize}
\item CPU requests individual WORDS from memory
\item Between cache and memory: Handle BLOCKS of data
\item \textbf{Block} = multiple consecutive words
\item Block size example: 8 bytes = 2 words (with 4-byte words)
\item Hidden from CPU (CPU still thinks in words)

\subsubsection{Why Blocks? (Spatial Locality)}

\paragraph{Instead of Words}

\begin{itemize}
\item Fetch single word CPU requested
\item Next access likely nearby address
\item Would require another fetch

\paragraph{Using Blocks}

\begin{itemize}
\item Fetch requested word AND nearby words together
\item Bring entire block (e.g., 8 consecutive bytes)
\item Subsequent accesses likely in same block (spatial locality)
\item Reduces future misses

\paragraph{Music Library Analogy}

\begin{itemize}
\item Want to listen to one song $\rightarrow$ Copy entire album to phone
\item Not just the single song you want right now
\item Because you'll likely want other songs from same album soon
\item Saves future copy operations

\paragraph{Block Benefits}

\begin{itemize}
\item Exploits spatial locality
\item Reduces miss rate
\item Amortizes fetch cost over multiple words
\item More efficient use of memory bandwidth

\subsubsection{Cache Management Decisions}

\paragraph{1. What to Keep in Cache}

\begin{itemize}
\item Based on BOTH locality principles
\item Recently accessed data (temporal locality)
\item Blocks containing nearby data (spatial locality)

\paragraph{2. What to Evict from Cache}

\begin{itemize}
\item Based on TEMPORAL locality
\item When cache full and need space for new block
\item Must throw out existing data

\subsubsection{Eviction Strategy (Ideal)}

\textbf{Least Recently Used (LRU):}

\begin{itemize}
\item Throw out LEAST RECENTLY USED (LRU) data
\item If cache has 10 blocks, need to evict 1
\item Choose the block that was used longest time ago
\item Keep more recently used blocks
\item Temporal locality suggests LRU block least likely to be accessed soon

\textbf{Example:}

\begin{itemize}
\item Cache has blocks A, B, C, D, E
\item Last access times: A(10 cycles ago), B(2 cycles ago), C(50 cycles ago), D(5 cycles ago), E(1 cycle ago)
\item Need to evict one block
\item Evict C (least recently used, 50 cycles ago)
\item Keep E, B, D, A (more recently used)

\subsection{Memory Addressing: Bytes, Words, and Blocks}

\subsubsection{Byte Address}

\textbf{Definition:} Address referring to individual byte in memory

\textbf{Characteristics:}

\begin{itemize}
\item Each byte-sized location has unique address
\item Standard memory addressing
\item Address Space: With 32-bit address, can access 2³² individual bytes

\textbf{Example Address:}

Address: 00000000000000000000000000001010 (binary)
       = 10 (decimal)
Points to: Byte at memory location 10

\textbf{Memory Structure:}

Address 0:  [byte 0]
Address 1:  [byte 1]
Address 2:  [byte 2]
...
Address 10: [byte 10]  $\leftarrow$ This byte addressed by example
...

\subsubsection{Word Address}

\textbf{Definition:} Address referring to a word (multiple bytes) in memory

\textbf{Typical Word Size:} 4 bytes (32 bits)

\paragraph{Word Alignment}

\begin{itemize}
\item Words start at addresses that are multiples of 4
\item Word 0: Addresses 0, 1, 2, 3
\item Word 1: Addresses 4, 5, 6, 7
\item Word 2: Addresses 8, 9, 10, 11
\item Word 3: Addresses 12, 13, 14, 15

\paragraph{Word Address Format (32-bit)}

[30-bit word identifier][2-bit byte offset]
                        └── Always "00" for word-aligned addresses

\textbf{Example:}

Address: ...00001000 (binary)
\begin{itemize}
\item Last 2 bits: 00 $\rightarrow$ Word-aligned
\item Remaining bits: Identify which word
\item This is address 8, start of word 2

\paragraph{Byte Within Word}

Last 2 bits select byte within word:

\begin{itemize}
\item \texttt{00} $\rightarrow$ First byte (address 8)
\item \texttt{01} $\rightarrow$ Second byte (address 9)
\item \texttt{10} $\rightarrow$ Third byte (address 10)
\item \texttt{11} $\rightarrow$ Fourth byte (address 11)

\textbf{Key Points:}

\begin{itemize}
\item Word addresses are multiples of 4
\item Can divide by 4 without remainder
\item Last 2 bits = 00 for word addresses
\item NOT all addresses ending in 00 are word addresses, but word addresses end in 00
\item Only portion of address except last 2 bits identifies the word

\subsubsection{Block Address}

\textbf{Definition:} Address referring to a block (multiple words) in memory

\textbf{Example Block Size:} 8 bytes = 2 words

\paragraph{Block Alignment}

\begin{itemize}
\item Blocks start at addresses that are multiples of 8
\item Block 0: Addresses 0-7
\item Block 1: Addresses 8-15
\item Block 2: Addresses 16-23
\item Block 3: Addresses 24-31

\paragraph{Block Address Format (32-bit)}

[Block Identifier][3-bit offset]
                   └── Last 3 bits for 8-byte blocks

\textbf{Example:}

Address: 00000000000000000000000000101101 (binary)
         = 45 (decimal)

Block Address Portion:
\begin{itemize}
\item Ignore last 3 bits: 00101 (offset part)
\item Block address: 00000000000000000000000000101 (identifies block)
\item This identifies the block containing address 45

\paragraph{Offset Within Block (3 bits for 8-byte blocks)}

\textbf{BYTE OFFSET (all 3 bits):}

\begin{itemize}
\item Used to identify individual BYTE within block
\item \texttt{000} $\rightarrow$ Byte 0
\item \texttt{001} $\rightarrow$ Byte 1
\item ...
\item \texttt{111} $\rightarrow$ Byte 7

\textbf{WORD OFFSET (most significant bit of offset):}

\begin{itemize}
\item Used to identify WORD within block (when block has 2 words)
\item \texttt{0XX} $\rightarrow$ First word (bytes 0-3)
\item \texttt{1XX} $\rightarrow$ Second word (bytes 4-7)
\item Only need 1 bit to select between 2 words

\subsubsection{Address Components Summary}

\textbf{For address with 8-byte blocks, 4-byte words:}

[Block Address][Word Offset][Byte in Word]
     ^              ^              ^
     |              |              └── 2 bits: Select byte within word
     |              └── 1 bit: Select word within block
     └── Remaining bits: Identify which block

\textbf{Example Breakdown:}

Address: ...00101101
\begin{itemize}
\item Last 2 bits (01): Byte offset within word $\rightarrow$ Byte 1 of word
\item 3rd bit from right (1): Word offset $\rightarrow$ Second word of block
\item Remaining bits (...00101): Block address $\rightarrow$ Block 5

\textbf{All Bytes in Same Block:}

\begin{itemize}
\item Share same block address
\item Differ only in offset bits

\textbf{Important Distinctions:}

\begin{itemize}
\item \textbf{Byte address:} Full 32 bits
\item \textbf{Word address:} Term refers to full address of word-aligned location
\item \textbf{Block address:} Term refers to portion of address identifying block (excluding offset)

\subsection{The Cache Addressing Problem}

\subsubsection{Problem Statement}

\paragraph{In Main Memory}

\begin{itemize}
\item Direct addressing: Address 10 $\rightarrow$ Direct access to location 10
\item Like array indexing: array[10] directly accesses index 10
\item Straightforward: Address uniquely identifies memory location
\item No search required: Hardware directly decodes address

\paragraph{In Cache}

\begin{itemize}
\item Cache is MUCH smaller than memory
\item Memory: Gigabytes (millions/billions of addresses)
\item Cache: Kilobytes or Megabytes (thousands/few million bytes)
\item Example: Memory has 1 million addresses, cache has only 8 slots

\subsubsection{The Challenge}

\begin{itemize}
\item CPU generates address from full address space (e.g., address 10)
\item Cache has only 8 slots (indices 0-7)
\item Cannot directly use memory address as cache index
\item Address 10 doesn't directly map to cache location
\item \textbf{How to find data in cache with memory address?}

\subsubsection{Initial Solution Idea: Store Addresses with Data}

\textbf{Approach:}

\begin{itemize}
\item Store memory address alongside data in cache
\item Each cache entry: [Address | Data]
\item When CPU requests address, search cache for matching address

\textbf{Problems with This Approach:}

\textbf{1. Space Overhead:}

\begin{itemize}
\item Must store full address (e.g., 32 bits) with each data block
\item Significant storage overhead
\item Example: 32-bit address + 256-bit data block = ~13% overhead

\textbf{2. Search Time:}

\begin{itemize}
\item Must search through ALL cache entries
\item Sequential or parallel search required
\item Example: 8 cache slots $\rightarrow$ Check all 8 tags
\item Time-consuming, degrades hit latency
\item Cannot directly access cache entry

\subsubsection{Need for Better Solution}

\textbf{Requirements:}

\begin{itemize}
\item Require MAPPING between memory addresses and cache locations
\item Want DIRECT access (no search) if possible
\item Must be efficient in both space and time

\textbf{Requirements for Practical Cache:}

\begin{enumerate}
\item Fast access (< 1 ns hit latency)
\item Minimal storage overhead
\item Direct or near-direct cache indexing
\item Efficient tag comparison (if needed)

\textbf{Solution Preview:} Address Mapping Functions

\begin{itemize}
\item Need function: Memory Address $\rightarrow$ Cache Location
\item Different mapping strategies possible
\item Simplest: Direct Mapping (discussed next)

\subsection{Direct-Mapped Cache}

\subsubsection{Direct Mapping Concept}

\textbf{Definition:}

\begin{itemize}
\item Each memory address maps to EXACTLY ONE cache location
\item One-to-one deterministic mapping
\item No choice in cache placement

\textbf{Mapping Rule:}

Cache Index = Block Address MOD (Number of Blocks in Cache)

\textbf{Formula:}

Cache Index = (Block Address) mod (Cache Size in Blocks)

\textbf{Example:}

\begin{itemize}
\item Cache has 8 blocks $\rightarrow$ Indices 0-7
\item Block address = 13
\item Cache index = 13 mod 8 = 5
\item Block 13 maps ONLY to cache index 5

\subsubsection{Mathematical Properties}

\paragraph{Mod Operation with Powers of 2}

\begin{itemize}
\item Cache sizes typically powers of 2 (1, 2, 4, 8, 16, 32, ...)
\item Mod by power of 2 = take least significant bits
\item Example: N mod 8 = N mod 2³ = last 3 bits of N

\paragraph{Hardware Implementation}

\begin{itemize}
\item No division circuit needed!
\item Simply extract least significant bits
\item Very fast, pure combinational logic

\subsubsection{Direct Mapping Example}

\textbf{Given:}

\begin{itemize}
\item Block size: 8 bytes
\item Cache size: 8 blocks
\item Cache indices: 0, 1, 2, 3, 4, 5, 6, 7

\textbf{Cache Structure (Initial View):}

| Index | Data Block |
| ----- | ---------- |
| 0     | [64 bits]  |
| 1     | [64 bits]  |
| 2     | [64 bits]  |
| 3     | [64 bits]  |
| 4     | [64 bits]  |
| 5     | [64 bits]  |
| 6     | [64 bits]  |
| 7     | [64 bits]  |

\textbf{Example Addresses:}

\textbf{Address 1:}

Binary: ...00000001[011]
         └─ Block address = 0
         └─ Offset = 3 bytes
Cache index = 0 mod 8 = 0
Maps to cache index 0

\textbf{Address 2 (block address in focus):}

Binary: ...00000101[000]
         └─ Block address = 5
         └─ Offset = 0
Cache index = 5 mod 8 = 5
Maps to cache index 5

\subsubsection{Address Structure for Direct-Mapped Cache}

[Tag][Index][Offset]
  ^     ^       ^
  |     |       └── Identifies byte/word within block
  |     └── Identifies cache location (index)
  └── Remaining bits to differentiate blocks mapping to same index

\paragraph{Bit Allocation (for 8-block cache, 8-byte blocks, 32-bit address)}

\begin{itemize}
\item \textbf{Offset:} 3 bits (for 8-byte blocks: 2³ = 8)
\item \textbf{Index:} 3 bits (for 8 cache blocks: 2³ = 8)
\item \textbf{Tag:} 26 bits (remaining: 32 - 3 - 3 = 26)

\paragraph{Index Bits}

\begin{itemize}
\item Least significant bits of block address
\item Directly select cache location
\item Number of bits = log₂(cache blocks)
\item 8 blocks $\rightarrow$ 3 index bits
\item 16 blocks $\rightarrow$ 4 index bits
\item 32 blocks $\rightarrow$ 5 index bits

\subsection{The Tag Problem in Direct-Mapped Cache}

\subsubsection{Conflict Issue}

\textbf{Multiple Blocks $\rightarrow$ Same Index:}

\begin{itemize}
\item Many memory blocks map to same cache index
\item Example: Blocks 5, 13, 21, 29, ... all map to index 5 (mod 8)
\item Only ONE can occupy cache index 5 at a time

\textbf{Example Addresses Mapping to Index 5:}

\textbf{Address A:}

Block address: ...00000101
Index bits (last 3): 101 $\rightarrow$ Index 5

\textbf{Address B:}

Block address: ...00001101
Index bits (last 3): 101 $\rightarrow$ Index 5

Both map to index 5, but different blocks!

\subsubsection{The Problem}

\begin{itemize}
\item When CPU requests address with index 5
\item Is data at index 5 for Address A or Address B?
\item Need way to differentiate between conflicting blocks

\subsubsection{Solution: TAG FIELD}

\textbf{Tag Definition:}

\begin{itemize}
\item Remaining bits of block address (excluding index and offset)
\item Stored WITH data in cache
\item Used to verify correct block is present

Tag = Block Address (excluding index bits)

\paragraph{Example Address Breakdown}

\textbf{Full Address:}

[26-bit Tag][3-bit Index][3-bit Offset]

\textbf{Address A:}

[00000000000000000000000000][101][000]
 └── Tag = 0                └─ Index=5 └─ Offset

\textbf{Address B:}

[00000000000000000000000001][101][000]
 └── Tag = 1                └─ Index=5 └─ Offset

Both have index 5, but DIFFERENT tags!

\subsubsection{Cache Structure with Tags}

| Index | Valid | Tag  | Data Block |
| ----- | ----- | ---- | ---------- |
| 0     | V     | Tag0 | [64 bits]  |
| 1     | V     | Tag1 | [64 bits]  |
| 2     | V     | Tag2 | [64 bits]  |
| 3     | V     | Tag3 | [64 bits]  |
| 4     | V     | Tag4 | [64 bits]  |
| 5     | V     | Tag5 | [64 bits]  |
| 6     | V     | Tag6 | [64 bits]  |
| 7     | V     | Tag7 | [64 bits]  |

\textbf{Storage Requirements Per Cache Entry:}

\begin{itemize}
\item Tag: 26 bits (in this example)
\item Valid bit: 1 bit
\item Data: 64 bits (8 bytes)
\item Total: 91 bits per entry

\textbf{Storage Overhead:}

Overhead = (Tag + Valid) / Total
         = (26 + 1) / (26 + 1 + 64)
         = 27 / 91
         ≈ 30% overhead in this small example

\paragraph{Note on Overhead}

\begin{itemize}
\item Example uses VERY small cache (8 blocks)
\item Real caches are much larger (thousands of blocks)
\item Larger caches $\rightarrow$ More index bits
\item More index bits $\rightarrow$ Fewer tag bits
\item Overhead percentage decreases with larger caches

\textbf{Example with Larger Cache:}

\begin{itemize}
\item 1024 blocks (2¹⁰)
\item Index: 10 bits
\item Tag: 32 - 10 - 3 = 19 bits
\item Overhead: (19+1)/84 ≈ 24% (better)

\subsubsection{Valid Bit}

\textbf{Purpose:}

\begin{itemize}
\item Indicates whether cache entry contains valid data
\item Prevents using uninitialized/stale data

\textbf{Initial State:}

\begin{itemize}
\item At program start, cache is empty
\item All entries contain garbage/random values
\item All valid bits set to 0 (invalid)

\textbf{After Data Loaded:}

\begin{itemize}
\item When block loaded into cache, valid bit set to 1
\item Indicates data is reliable

\textbf{Uses Beyond Initialization:}

\begin{itemize}
\item Cache coherence (multi-processor systems)
\item Invalidating stale data
\item Handling context switches

\subsection{Cache Read Access Operation}

\subsubsection{Read Access Process}

\textbf{CPU Provides:}

\begin{enumerate}
\item Address (word or byte address)
\item Control Signal: Read/Write indicator (from control unit)

\subsubsection{For Read Access}

\paragraph{Step 1: ADDRESS BREAKDOWN}

\begin{itemize}
\item Receive address from CPU
\item Parse into three fields:
\item Tag bits
\item Index bits
\item Offset bits

\textbf{Example Address (32-bit):}

[26-bit Tag][3-bit Index][3-bit Offset]

\paragraph{Step 2: INDEXING THE CACHE}

\begin{itemize}
\item Extract index bits from address
\item Use index to directly access cache entry
\item Combinational logic routes to correct entry
\item Like array indexing: index 5 $\rightarrow$ entry 5
\item No search needed!
\item Fast: Pure combinational delay

\textbf{Hardware:}

\begin{itemize}
\item Decoder circuit takes index bits
\item Selects one of N cache entries
\item Activates corresponding row

\paragraph{Step 3: TAG COMPARISON}

\begin{itemize}
\item Extract stored tag from selected cache entry
\item Extract tag bits from incoming address
\item Compare the two tags
\item Use comparator circuit

\textbf{Comparator Circuit:}

\begin{itemize}
\item For each bit position: XNOR gate
\item XNOR outputs 1 if bits match, 0 if different
\item AND all XNOR outputs together
\item Final output: 1 if all bits match (tags equal), 0 otherwise

\textbf{Example (4-bit tags):}

Stored tag:   1 0 1 1
Address tag:  1 0 1 1
XNOR:         1 1 1 1  $\rightarrow$ AND = 1 (MATCH!)

Stored tag:   1 0 1 1
Address tag:  1 0 0 1
XNOR:         1 1 0 1  $\rightarrow$ AND = 0 (NO MATCH)

\textbf{For N-bit tag:}

\begin{itemize}
\item N XNOR gates (parallel)
\item 1 N-input AND gate
\item Very fast combinational circuit

\paragraph{Step 4: VALID BIT CHECK}

\begin{itemize}
\item Extract valid bit from selected cache entry
\item Check if entry is valid
\item Valid bit = 1 $\rightarrow$ Entry contains valid data
\item Valid bit = 0 $\rightarrow$ Entry is invalid (ignore)

\paragraph{Step 5: HIT/MISS DETERMINATION}

\begin{itemize}
\item Combine tag comparison and valid bit
\item Hit = (Tag Match) AND (Valid Bit = 1)
\item Miss = (Tag Mismatch) OR (Valid Bit = 0)

\textbf{Logic Circuit:}

Tag Match Output ─┐
                  AND ─$\rightarrow$ Hit/Miss Signal
Valid Bit ────────┘

\textbf{Output:}

\begin{itemize}
\item 1 $\rightarrow$ HIT (data present and valid)
\item 0 $\rightarrow$ MISS (data not present or invalid)

\textbf{Hit Latency:}

\begin{itemize}
\item Time for steps 2-5
\item Dominated by:
\item Indexing combinational delay
\item Tag comparator delay
\item Valid bit access
\item Typically < 1 nanosecond for SRAM

\paragraph{Step 6: DATA EXTRACTION (Parallel with Tag Check)}

\begin{itemize}
\item Can happen in PARALLEL with tag comparison
\item Extract entire data block from selected cache entry
\item Put data block on internal wires

\textbf{Data Block:}

\begin{itemize}
\item Contains multiple words
\item Example: 8 bytes = 2 words (4 bytes each)

\paragraph{Step 7: WORD SELECTION (Using Offset)}

\begin{itemize}
\item CPU wants a single WORD, not entire block
\item Use offset bits to select correct word from block
\item Offset bits $\rightarrow$ Multiplexer select signal

\textbf{Multiplexer (MUX):}

\begin{itemize}
\item Inputs: All words in the data block
\item Select: Word offset bits from address
\item Output: Selected word

\textbf{Example (2 words per block):}

\begin{itemize}
\item Block contains: Word0 (bytes 0-3), Word1 (bytes 4-7)
\item Word offset = 0 $\rightarrow$ Select Word0
\item Word offset = 1 $\rightarrow$ Select Word1
\item Need 1-bit select for 2:1 MUX

\textbf{Example (4 words per block):}

\begin{itemize}
\item Block contains: Word0, Word1, Word2, Word3
\item Word offset = 2 bits $\rightarrow$ Select among 4 words
\item Need 4:1 MUX

\textbf{Timing:}

\begin{itemize}
\item Data extraction and word selection happen in parallel with tag check
\item Both combinational circuits
\item Similar delays
\item Can overlap operations

\paragraph{Step 8: DECISION BASED ON HIT/MISS}

\textbf{If HIT (signal = 1):}

\begin{itemize}
\item Selected word is correct data
\item Send word to CPU immediately
\item Access complete
\item Total time: Hit latency (< 1 ns)

\textbf{If MISS (signal = 0):}

\begin{itemize}
\item Selected word is WRONG data (different block or invalid)
\item CANNOT send to CPU
\item Must fetch correct block from main memory (DRAM)
\item CPU must STALL (wait)
\item Cache controller takes over
\item Total time: Hit latency + Miss penalty

\textbf{Miss Handling:}

\begin{itemize}
\item Will discuss in next lecture
\item Involves accessing main memory
\item Bringing block into cache
\item Potentially evicting old block
\item Then serving CPU request

\subsection{Cache Circuit Components Summary}

\subsubsection{Key Circuit Elements}

\paragraph{1. INDEXING CIRCUITRY}

\begin{itemize}
\item \textbf{Input:} Index bits from address
\item \textbf{Function:} Decoder to select cache entry
\item \textbf{Output:} Activates one cache row
\item \textbf{Type:} Combinational logic
\item \textbf{Delay:} Part of hit latency

\paragraph{2. TAG COMPARATOR}

\begin{itemize}
\item \textbf{Input:} Stored tag, Address tag
\item \textbf{Function:} Multi-bit equality check
\item \textbf{Components:}
\item N XNOR gates (N = tag bit width)
\item 1 N-input AND gate
\item \textbf{Output:} 1 if equal, 0 if not equal
\item \textbf{Type:} Combinational logic
\item \textbf{Delay:} Part of hit latency

\paragraph{3. VALID BIT CHECK}

\begin{itemize}
\item \textbf{Input:} Valid bit from cache entry
\item \textbf{Function:} Read and check validity
\item \textbf{Output:} 1 if valid, 0 if invalid
\item \textbf{Type:} Simple wire/buffer
\item \textbf{Delay:} Minimal

\paragraph{4. HIT/MISS LOGIC}

\begin{itemize}
\item \textbf{Input:} Tag match signal, Valid bit
\item \textbf{Function:} AND gate
\item \textbf{Output:} Hit/Miss signal
\item \textbf{Type:} Combinational logic
\item \textbf{Delay:} Single gate delay

\paragraph{5. DATA ARRAY ACCESS}

\begin{itemize}
\item \textbf{Input:} Index bits
\item \textbf{Function:} Read data block from cache
\item \textbf{Output:} Multi-word data block
\item \textbf{Type:} SRAM memory read
\item \textbf{Delay:} SRAM access time (parallel with tag check)

\paragraph{6. WORD SELECTOR (Multiplexer)}

\begin{itemize}
\item \textbf{Input:} Data block, Word offset bits
\item \textbf{Function:} Select one word from block
\item \textbf{Output:} Single word
\item \textbf{Type:} MUX (combinational)
\item \textbf{Delay:} MUX delay (parallel with tag check)

\paragraph{7. CONTROL LOGIC (Cache Controller)}

\begin{itemize}
\item \textbf{Input:} Hit/Miss signal, Read/Write control
\item \textbf{Function:} Decide next actions
\item \textbf{Output:} Control signals for CPU, memory
\item \textbf{On Hit:} Enable data to CPU
\item \textbf{On Miss:} Initiate memory fetch, stall CPU
\item \textbf{Type:} Sequential logic (state machine)

\subsubsection{Hit Latency Components}

\textbf{Contributing Factors:}

\begin{itemize}
\item Indexing delay
\item Tag comparison delay
\item Valid bit check delay
\item Hit/Miss determination delay
\item Word selection delay (parallel)
\item Wire delays

\textbf{Dominant Delays:}

\begin{itemize}
\item Indexing (decoder)
\item Tag comparator (XNOR + AND)
\item These determine critical path

\textbf{Parallelism:}

\begin{itemize}
\item Tag check and data extraction happen simultaneously
\item Reduces total hit latency
\item Only one path delay counts (whichever is longer)

\subsection{Next Lecture Preview}

\subsubsection{Topics to Cover}

\paragraph{1. Cache Miss Handling}

\begin{itemize}
\item What happens after miss is determined?
\item How to fetch block from main memory?
\item Where to place new block in cache?
\item What to do if cache location occupied?

\paragraph{2. Cache Controller State Machine}

\begin{itemize}
\item Not just combinational logic
\item Sequential control needed for misses
\item Multiple clock cycles to handle miss
\item States: Idle, Compare Tags, Allocate, Write Back, etc.

\paragraph{3. Write Operations}

\begin{itemize}
\item Read operation covered this lecture
\item Write more complex: Must update cache AND memory
\item Write policies: Write-through, Write-back
\item Dirty bits for modified blocks

\paragraph{4. Replacement Policies}

\begin{itemize}
\item When cache full, which block to evict?
\item Least Recently Used (LRU)
\item Other policies: FIFO, Random, LFU

\paragraph{5. Performance Analysis}

\begin{itemize}
\item Calculate average access time
\item Impact of hit rate, miss penalty
\item Cache size vs. performance tradeoffs

\paragraph{6. Advanced Cache Concepts}

\begin{itemize}
\item Set-associative caches (beyond direct-mapped)
\item Multi-level caches (L1, L2, L3)
\item Fully associative caches

\subsection{Key Takeaways and Summary}

\subsubsection{Historical Foundations}

\begin{itemize}
\item Early computers had no memory/software concept
\item Alan Turing conceived stored program computer (1936)
\item John von Neumann implemented it in EDVAC (1948)
\item Von Neumann architecture: Unified memory for instructions and data
\item Harvard architecture: Separate instruction and data memories

\subsubsection{Memory Technologies Hierarchy}

| Technology | Speed             | Size             | Cost                      |
| ---------- | ----------------- | ---------------- | ------------------------- |
| SRAM       | Fastest (< 1 ns)  | Smallest (KB-MB) | Most expensive ($2000/GB) |
| DRAM       | Medium (~50 ns)   | Medium (GB)      | Moderate ($10/GB)         |
| Flash      | Similar to DRAM   | Gigabytes        | Cheap (< $1/GB)           |
| Disk       | Slowest (5-10 ms) | Largest (TB)     | Cheapest (cents/GB)       |

\subsubsection{The Performance Problem}

\begin{itemize}
\item CPU cycle time: < 1 nanosecond
\item Main memory cycle time: ~50 nanoseconds
\item \textbf{Memory 50$\times$ slower than CPU!}
\item Pipeline requires memory access in 1 cycle
\item Cannot directly use DRAM for CPU memory accesses

\subsubsection{Memory Hierarchy Solution}

\begin{itemize}
\item Multiple levels: SRAM (cache) $\rightarrow$ DRAM $\rightarrow$ Disk
\item CPU accesses only top level (cache)
\item Upper levels hold subsets of lower levels
\item Trick CPU: Fast as SRAM, large as Disk
\item Requires very high hit rate (> 99.9%) at cache level

\subsubsection{Principles of Locality}

\textbf{1. Temporal Locality:} Recently accessed data likely accessed again soon

\begin{itemize}
\item Example: Loop variables, instructions in loops

\textbf{2. Spatial Locality:} Data near recently accessed data likely accessed soon

\begin{itemize}
\item Example: Array elements, sequential instructions

\begin{itemize}
\item Both principles present in virtually all programs
\item Foundation for cache effectiveness

\subsubsection{Memory Addressing}

\begin{itemize}
\item \textbf{Byte Address:} Individual byte reference (full address)
\item \textbf{Word Address:} 4-byte word reference (last 2 bits = 00 for alignment)
\item \textbf{Block Address:} Multiple-word block reference (excludes offset bits)
\item Address structure: [Block Address][Offset]
\item Offset subdivides: [Word Offset][Byte in Word]

\subsubsection{Cache Terminology}

\begin{itemize}
\item \textbf{Hit:} Data found in cache $\rightarrow$ Fast access (< 1 ns)
\item \textbf{Miss:} Data not in cache $\rightarrow$ Slow access (+ ~100 ns penalty)
\item \textbf{Hit Rate:} Fraction of accesses that hit (want > 99.9%)
\item \textbf{Miss Rate:} Fraction of accesses that miss (1 - Hit Rate)
\item \textbf{Hit Latency:} Time to determine hit and access data
\item \textbf{Miss Penalty:} EXTRA time to fetch from memory on miss

\subsubsection{Cache Organization (Direct-Mapped)}

\begin{itemize}
\item Each memory block maps to exactly ONE cache location
\item Mapping: Cache Index = Block Address mod (Cache Size)
\item Address fields: [Tag][Index][Offset]
\item \textbf{Index:} Selects cache entry directly (no search!)
\item \textbf{Tag:} Differentiates blocks mapping to same index
\item \textbf{Offset:} Selects word/byte within block
\item \textbf{Valid bit:} Indicates if entry contains valid data

\subsubsection{Direct-Mapped Cache Structure}

\begin{itemize}
\item \textbf{Tag array:} Stores tags for verification
\item \textbf{Valid bit array:} Validity indicators
\item \textbf{Data array:} Stores actual data blocks
\item Index not stored (implicit in position)

\subsubsection{Cache Read Access Process}

\begin{enumerate}
\item Extract index from address $\rightarrow$ Access cache entry
\item Extract tag from cache entry $\rightarrow$ Compare with address tag
\item Check valid bit from entry
\item Determine hit/miss: (Tag Match) AND (Valid)
\item In parallel: Extract data block, select word using offset
\item If HIT: Send word to CPU (done in < 1 ns)
\item If MISS: Must fetch from memory (will cover next lecture)

\subsubsection{Critical Requirements}

\begin{itemize}
\item Hit latency must be < 1 CPU clock cycle
\item Hit rate must be very high (> 99.9%)
\item Only way to achieve: Exploit locality principles
\item Direct mapping enables fast indexing (no search)
\item Parallel tag check and data extraction minimize latency

\subsubsection{Average Access Time Formula}

Average Access Time = Hit Latency + (Miss Rate $\times$ Miss Penalty)

\begin{itemize}
\item Must keep Miss Rate very low for performance
\item Even 1% miss rate catastrophic if penalty is 100$\times$
\item Example: 1% miss rate $\rightarrow$ 1 + (0.01 $\times$ 100) = 2 ns average
\item Example: 0.1% miss rate $\rightarrow$ 1 + (0.001 $\times$ 100) = 1.1 ns average
\item Target: 99.9% or better hit rate

\subsubsection{Pending Topics (Next Lectures)}

\begin{itemize}
\item Cache miss handling and memory fetch
\item Cache controller state machine
\item Write operations and write policies
\item Block replacement strategies (LRU, etc.)
\item Set-associative and fully associative caches
\item Multi-level cache hierarchies
\item Performance analysis and optimization

\subsubsection{Music Library Analogy Summary}

\begin{itemize}
\item \textbf{Phone (cache):} Small, fast, always accessible
\item \textbf{Computer (main memory):} Larger, slower, main collection
\item \textbf{Internet (disk):} Huge, slowest, everything available
\item Listen from phone (CPU accesses cache)
\item Copy from computer when song not on phone (fetch on miss)
\item Download from internet when not on computer (fetch from disk)
\item Keep favorite songs on phone (exploit temporal locality)
\item Copy whole album at once (exploit spatial locality)

\subsection{Key Takeaways}

\begin{enumerate}
\item \textbf{Stored-program concept} revolutionized computing—programs stored in memory like data, eliminating manual reconfiguration for each algorithm.

\begin{enumerate}
\item \textbf{Von Neumann architecture} established fundamental computer organization—CPU, memory, and I/O with instructions and data sharing same memory.

\begin{enumerate}
\item \textbf{Processor-memory speed gap} creates performance bottleneck—CPU operates at nanosecond scale while main memory requires tens of nanoseconds.

\begin{enumerate}
\item \textbf{Memory hierarchy} provides illusion of large, fast memory—small fast cache near CPU, larger slower DRAM main memory, massive slow disk storage.

\begin{enumerate}
\item \textbf{Temporal locality}: Recently accessed data likely accessed again soon—programs exhibit loops, function calls, and repeated variable access patterns.

\begin{enumerate}
\item \textbf{Spatial locality}: Nearby data likely accessed soon—programs access arrays sequentially and instructions execute in order.

\begin{enumerate}
\item \textbf{Cache exploits locality} to achieve high hit rates—keeping frequently accessed data in fast storage dramatically improves average access time.

\begin{enumerate}
\item \textbf{Cache organized in blocks}, not individual words—exploiting spatial locality by fetching multiple words together.

\begin{enumerate}
\item \textbf{Direct-mapped cache}: Each memory block maps to exactly one cache location—simplest cache organization using modulo arithmetic for mapping.

10. \textbf{Address breakdown}: Tag + Index + Offset—index selects cache entry, tag identifies specific block, offset selects word within block.

11. \textbf{Valid bit} indicates cache entry contains meaningful data—essential for distinguishing real data from uninitialized entries at startup.

12. \textbf{Cache hit} occurs when requested data found in cache—CPU receives data in ~1 nanosecond, avoiding slow main memory access.

13. \textbf{Cache miss} requires main memory fetch—takes ~100 nanoseconds, replacing cache entry with new block from memory.

14. \textbf{Hit rate determines cache effectiveness}—even 1% miss rate significantly impacts average memory access time with 100$\times$ penalty.

15. \textbf{Block size affects performance}—larger blocks exploit spatial locality better but reduce total number of blocks, potentially increasing conflicts.

16. \textbf{Cache size} represents total data storage capacity—typical L1 caches 32-64 KB, L2 caches 256 KB-1 MB.

17. \textbf{Tag comparison} happens in parallel with data access—enabling fast hit detection and maintaining single-cycle cache access.

18. \textbf{Music library analogy} clarifies cache concept—phone (cache) holds favorites, computer (DRAM) has main collection, internet (disk) contains everything.

19. \textbf{Cache transparent to programmer}—software sees uniform memory, hardware manages cache automatically for best performance.

20. \textbf{Memory hierarchy only works because programs exhibit locality}—without temporal and spatial locality, caching would fail catastrophically.

\subsection{Summary}

The introduction to memory systems and cache memory reveals how the fundamental processor-memory speed gap—with CPUs operating 100$\times$ faster than main memory—drives sophisticated cache hierarchy designs that create the illusion of large, fast memory. Historical context from Alan Turing's theoretical foundations through Von Neumann's stored-program architecture establishes how modern computers execute instructions fetched from memory rather than requiring manual reconfiguration. The memory hierarchy concept, with small fast SRAM caches near the CPU, larger slower DRAM main memory, and massive disk storage, exploits two fundamental program properties: temporal locality (recently accessed data likely accessed again soon) and spatial locality (nearby data likely accessed soon). Cache memory, organized in blocks rather than individual words, dramatically improves average access time by maintaining frequently accessed data in fast storage, achieving hit rates often exceeding 95% in practice. Direct-mapped cache organization, the simplest mapping scheme, uses modulo arithmetic to assign each memory block to exactly one cache location, with address bits divided into tag (identifying specific block), index (selecting cache entry), and offset (choosing word within block). The valid bit distinguishes real cached data from uninitialized entries, essential at system startup when cache contains random values. Cache hits deliver data in approximately 1 nanosecond while misses require ~100 nanosecond main memory access, making even small miss rates significant—a 1% miss rate doubles average access time from 1 ns to 2 ns. The music library analogy effectively clarifies concepts: phone storage represents cache (small, fast, always accessible), computer storage represents main memory (larger, slower, main collection), and internet streaming represents disk (unlimited, very slow, backup). This cache transparency—programmer sees uniform memory while hardware automatically manages caching—enables software compatibility across different cache configurations. The critical insight remains that memory hierarchy effectiveness depends entirely on programs exhibiting locality; without these natural access patterns inherent to how we write code, caching would provide no benefit. Understanding cache fundamentals proves essential for both hardware designers optimizing cache architectures and software developers writing cache-friendly code that maximizes hit rates.
