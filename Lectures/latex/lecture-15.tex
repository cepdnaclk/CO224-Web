\section{Lecture 15: Cache Memory Operations – Read/Write Access and Write Policies}

\emph{By Dr. Isuru Nawinne}

\subsection{Introduction}

This lecture provides a comprehensive, step‑by‑step examination of how a direct‑mapped cache services read and write requests, differentiates hits from misses, and preserves data correctness. We finish the full read path (including stall + block fetch sequence), analyze write hits and misses, and introduce the write‑through policy as the simplest consistency mechanism between cache and main memory. Performance consequences of constant memory writes, the need for high hit rates, and the motivation for more advanced write‑back policies (next lecture) are emphasized. By the end you will understand exactly what the cache controller must do (state transitions, signals, data/tag/valid updates) for every access type and why write policies are a central architectural tradeoff.

\subsection{Lecture Introduction and Recap}

\subsubsection{Previous Lecture Review}

\paragraph{Memory Systems Foundation}

\begin{itemize}
\item Memory hierarchy concept (SRAM $\rightarrow$ DRAM $\rightarrow$ Disk)
\item Illusion of large and fast memory simultaneously
\item CPU accesses only cache (top level)

\paragraph{Locality Principles}

\begin{itemize}
\item \textbf{Temporal locality:} Recently accessed data likely accessed again soon
\item \textbf{Spatial locality:} Nearby data likely accessed soon
\item Foundation for cache effectiveness

\paragraph{Direct-Mapped Cache Introduction}

\begin{itemize}
\item Each memory block maps to exactly ONE cache location
\item Mapping function: Cache Index = Block Address MOD Cache Size
\item Read access process partially covered

\paragraph{Cache Structure (Recap)}

\begin{itemize}
\item \textbf{Data array:} Stores data blocks (not individual words)
\item \textbf{Tag array:} Stores tags for block identification
\item \textbf{Valid bit array:} Indicates valid/invalid entries
\item \textbf{Index:} Not stored, implicit in position (for convenience in diagrams)

\paragraph{Address Breakdown (Recap)}

[Tag][Index][Offset]
  ^      ^       ^
  |      |       └── Identifies word/byte within block
  |      └── Identifies cache entry (direct mapping)
  └── Remaining bits for block identification

\subsubsection{Today's Focus}

\begin{itemize}
\item Complete discussion of read miss handling
\item Write access operations (hit and miss)
\item Write policies and their implications
\item Data consistency issues
\item Performance considerations

\subsection{Cache Read Access - Complete Process}

\subsubsection{Read Access Input Signals}

\textbf{From CPU to Cache Controller:}

\begin{enumerate}
\item \textbf{Address} (word or byte address)
\item \textbf{Read Control Signal} (from CPU control unit)
\item Indicates this is a read operation (not write)
\item Part of memory control signals

\subsubsection{Cache Read Steps (Detailed)}

\paragraph{Step 1: Address Decomposition}

\begin{itemize}
\item Parse incoming address into three fields:
\item \textbf{Tag:} For verification
\item \textbf{Index:} For cache entry selection
\item \textbf{Offset:} For word/byte selection within block

\paragraph{Step 2: Cache Entry Selection (Indexing)}

\begin{itemize}
\item Extract index bits from address
\item Cache controller knows which bits are index (by design)
\item Use demultiplexer circuitry to access correct cache entry
\item Example: Index = 101 (binary) $\rightarrow$ Access cache entry 5
\item Direct access, no search needed
\item Combinational logic (fast)

\paragraph{Step 3: Tag Comparison}

\begin{itemize}
\item Extract stored tag from selected cache entry
\item Extract tag from incoming address
\item Use comparator circuit (XNOR gates + AND gate)
\item Output: 1 if tags match, 0 if tags differ

\paragraph{Step 4: Valid Bit Check}

\begin{itemize}
\item Extract valid bit from selected cache entry
\item Check if entry contains valid data
\item Output: 1 if valid, 0 if invalid

\paragraph{Step 5: Hit/Miss Determination}

\begin{itemize}
\item Logic: \textbf{Hit = (Tag Match) AND (Valid Bit)}
\item If both conditions true $\rightarrow$ HIT
\item If either condition false $\rightarrow$ MISS
\item Single AND gate combines both signals

\paragraph{Step 6: Data Extraction (Parallel Operation)}

\begin{itemize}
\item Happens simultaneously with tag comparison
\item Extract entire data block from cache entry
\item Place data block on internal wires
\item Example: 8-byte block = 2 words

\paragraph{Step 7: Word Selection (Using Offset)}

\begin{itemize}
\item CPU requests a single WORD
\item Use word offset bits as MUX select signal
\item Example: 2 words in block
\item Offset MSB = 0 $\rightarrow$ Select first word
\item Offset MSB = 1 $\rightarrow$ Select second word
\item Multiplexer extracts correct word from block

\subsubsection{Timing Optimization}

\textbf{Parallel Operations:}

\begin{itemize}
\item Tag comparison (Steps 3-5) and data extraction (Steps 6-7) happen in PARALLEL
\item Both are combinational circuits
\item Total delay = max(tag comparison delay, data extraction delay)
\item Reduces overall hit latency

\subsubsection{Read Hit Outcome}

\begin{itemize}
\item Selected word is correct data
\item Send word to CPU immediately
\item No stall required
\item Total time: Hit latency (< 1 nanosecond for SRAM)
\item Completes within one CPU clock cycle
\item Pipeline continues uninterrupted

\subsubsection{Pipeline Integration}

\begin{itemize}
\item In MIPS pipeline, MEM stage accesses memory
\item With cache hit: Memory access completes in 1 cycle
\item Pipeline maintains smooth operation
\item No bubbles inserted

\subsection{Cache Read Miss Handling}

\subsubsection{Read Miss Scenario}

\paragraph{Miss Conditions}

\begin{enumerate}
\item \textbf{Tag mismatch} (most common)
\item Requested block not in cache
\item Different block occupies that cache location
\item \textbf{Invalid entry}
\item Valid bit = 0
\item Entry contains no valid data (e.g., after initialization)
\item \textbf{Both conditions}
\item Tag mismatch AND invalid entry

\subsubsection{Read Miss Response Required Actions}

\paragraph{Action 1: STALL THE CPU}

\textbf{Process:}

\begin{itemize}
\item CPU cannot proceed without requested data
\item Data hazard would occur if CPU continues
\item Cache controller sends STALL signal to CPU
\item CPU must monitor stall signal continuously
\item When stall signal high $\rightarrow$ Freeze CPU operation
\item Stop fetching new instructions
\item Freeze all pipeline stages
\item Hold current state

\textbf{CPU's Perspective:}

\begin{itemize}
\item CPU doesn't know cache and memory are separate
\item CPU sees memory hierarchy as single "memory"
\item Must respond to stall signal from memory subsystem
\item In MEM stage: Check and respond to stall signal

\paragraph{Action 2: MAKE READ REQUEST TO MAIN MEMORY}

\textbf{Request Details:}

\begin{itemize}
\item Request the missing DATA BLOCK (not just word!)
\item Cache and memory trade in BLOCKS
\item CPU trades in words/bytes, but cache-memory interface uses blocks
\item Send block address to main memory
\item Memory fetches entire block

\textbf{Reason for Block Transfer:}

\begin{itemize}
\item Exploits spatial locality
\item Fetches requested word AND nearby words
\item Reduces future misses for nearby addresses
\item More efficient than fetching single words

\textbf{Memory Access Time:}

\begin{itemize}
\item DRAM access: Several CPU clock cycles
\item Range: 10 to 100+ CPU clock cycles
\item Much slower than cache (< 1 cycle)
\item This is the \textbf{MISS PENALTY}

\paragraph{Action 3: WAIT FOR MEMORY RESPONSE}

\begin{itemize}
\item Memory performs read operation
\item Data travels from memory to cache
\item Controller waits (CPU still stalled)
\item Multiple clock cycles elapse

\paragraph{Action 4: UPDATE CACHE ENTRY}

\textbf{Three components to update:}

\textbf{a) Update Data Block:}

\begin{itemize}
\item Write fetched block into cache entry
\item Replace old data at that index

\textbf{b) Update Tag:}

\begin{itemize}
\item Extract tag from block address
\item Write tag into tag array at that index
\item Ensures future tag comparisons work correctly

\textbf{c) Set Valid Bit:}

\begin{itemize}
\item Set valid bit to 1
\item Denotes entry now contains valid data

\paragraph{Action 5: SEND DATA TO CPU}

\begin{itemize}
\item Extract requested word from newly loaded block
\item Use offset to select correct word
\item Put data on bus to CPU
\item CPU receives requested data

\paragraph{Action 6: CLEAR STALL SIGNAL}

\begin{itemize}
\item Cache controller clears (lowers) stall signal
\item CPU detects stall signal going low
\item CPU resumes operation
\item Pipeline unfreezes and continues

\subsubsection{Total Read Miss Time}

\textbf{Formula:}

Read Miss Time = Hit Latency + Miss Penalty

\textbf{Where:}

\begin{itemize}
\item \textbf{Hit Latency:} Time to determine it's a miss (< 1 ns)
\item \textbf{Miss Penalty:} Time to fetch from memory (10-100+ CPU cycles)

\textbf{Example Calculation:}

\begin{itemize}
\item Hit latency: 1 ns (1 cycle at 1 GHz)
\item Miss penalty: 50 ns (50 cycles at 1 GHz)
\item Total: 1 + 50 = 51 cycles

\subsubsection{Performance Impact}

\begin{itemize}
\item Single miss causes 50+ cycle stall
\item Catastrophic for pipeline performance
\item Emphasizes need for high hit rate (> 99.9%)

\subsubsection{Question: What About the Old Block?}

\textbf{The Deferred Question:}

\begin{itemize}
\item When fetching new block on miss
\item Old block occupies that cache entry
\item What happens to old block?
\item Is it okay to discard it?

\textbf{Initial Answer:} "We'll discuss after introducing write policies"

\begin{itemize}
\item Answer depends on write policy
\item Need to understand writes first
\item Question will be revisited

\subsection{Cache Write Access - Introduction}

\subsubsection{Write Access Input Signals}

\textbf{From CPU to Cache Controller:}

\begin{enumerate}
\item \textbf{Address} (where to write)
\item \textbf{Data Word} (what to write)
\item \textbf{Write Control Signal} (indicates write operation)

Three inputs vs. two for read (no data input needed for read).

\subsubsection{Write Access Process}

\paragraph{Step 1: Address Decomposition}

\begin{itemize}
\item Same as read: [Tag][Index][Offset]

\paragraph{Step 2: Cache Entry Selection}

\begin{itemize}
\item Same as read: Use index bits
\item Demultiplexer accesses correct entry
\item Direct access based on index
\item Example: Index 101 $\rightarrow$ Entry 5

\paragraph{Step 3: Tag Comparison}

\begin{itemize}
\item Extract tag from cache entry
\item Compare with incoming address tag
\item Comparator circuit (same as read)
\item Output: Match or no match

\paragraph{Step 4: Valid Bit Check}

\begin{itemize}
\item Extract and check valid bit
\item Same as read operation
\item Ensures entry is valid

\paragraph{Step 5: Hit/Miss Determination}

\begin{itemize}
\item Hit = (Tag Match) AND (Valid Bit)
\item Same logic as read
\item Determines write hit or write miss

\paragraph{Step 6: Data Writing (The Difference)}

\textbf{This is where write differs from read:}

\begin{itemize}
\item Must write data word to correct location in block
\item Use offset to determine which word in block

\subsubsection{Writing Mechanism}

\textbf{Input:}

\begin{itemize}
\item Incoming data word (from CPU)
\item Offset bits from address

\textbf{Demultiplexer Selection:}

\begin{itemize}
\item Use word offset as demultiplexer select signal
\item Example with 2 words per block:
\item Word offset = 0 $\rightarrow$ Write to first word
\item Word offset = 1 $\rightarrow$ Write to second word
\item Demultiplexer directs data to correct word position

\textbf{Example:}

\begin{itemize}
\item Block has 2 words: Word0 (bytes 0-3), Word1 (bytes 4-7)
\item Incoming data word: 0x12345678
\item Offset MSB = 1 $\rightarrow$ Select Word1
\item Demux directs data to Word1 position in block

\textbf{Write Operation Control:}

\begin{itemize}
\item Writing controlled by Write control signal from CPU
\item Only write if signal indicates write operation
\item Demultiplexer enabled by write signal

\subsubsection{Critical Question: Can Write and Tag Compare Happen in Parallel?}

\paragraph{For Read (Previous Discussion)}

\begin{itemize}
\item \textbf{YES, both can happen in parallel}
\item If miss, discard extracted data (no harm done)
\item Reading doesn't change cache state

\paragraph{For Write (Current Question)}

\textbf{More problematic!}

\begin{itemize}
\item What if we write and then discover tag mismatch?

\textbf{Scenario:}

\begin{itemize}
\item Write to cache entry simultaneously with tag comparison
\item Tag comparison returns MISMATCH
\item We've now CORRUPTED data in cache!
\item Written to wrong block (different tag)
\item Data integrity violated

\textbf{Problem:}

\begin{itemize}
\item If invalid entry: Not too serious (data was garbage anyway)
\item If tag mismatch: \textbf{SERIOUS problem!}
\item Overwrote valid data for different block
\item That block's data now corrupted
\item Future accesses to that block get wrong data

\textbf{Initial Conclusion:}

\begin{itemize}
\item Cannot safely write and tag compare in parallel
\item Need mechanism to prevent corruption
\item Solution depends on write policy (discussed next)

\subsection{Write Policies - Introduction}

\subsubsection{The Data Consistency Problem}

\textbf{Scenario:}

\begin{itemize}
\item CPU writes to address A
\item Address A hits in cache
\item Cache controller writes new value to cache entry
\item Cache now has updated value
\item Main memory still has OLD value
\item Two versions exist: Cache version $\neq$ Memory version

\textbf{The Inconsistency:}

\begin{itemize}
\item Cache entry now INCONSISTENT with main memory
\item Same address has different values in different levels
\item Data coherence problem

\subsubsection{Why This Matters}

\begin{itemize}
\item Future access to same address: Which value is correct?
\item If cache entry replaced: New value lost
\item I/O devices may access memory directly (bypass cache)
\item Multi-processor systems: Other CPUs access memory
\item Must maintain data consistency across hierarchy

\subsubsection{Two Fundamental Write Policies}

\begin{enumerate}
\item \textbf{Write-Through} (discussed this lecture)
\item \textbf{Write-Back} (mentioned, detailed in next lecture)

\subsection{Write-Through Policy}

\subsubsection{Write-Through Definition}

\textbf{Policy Statement:}

> "Always write to BOTH cache AND memory"

\textbf{Mechanism:}

\begin{itemize}
\item On every write operation:
\end{itemize}
\begin{enumerate}
\item Write to cache (if hit)
\item Simultaneously write to main memory
\item Both levels updated together
\item Ensures cache and memory always consistent

\subsubsection{Write-Through Process}

\paragraph{Write Hit with Write-Through}

\begin{enumerate}
\item Determine it's a write hit (tag match + valid)
\item Write data word to cache block (using offset)
\item Also send write request to main memory
\item Update same address in memory
\item Wait for memory write to complete
\item Both cache and memory now have same value

\paragraph{Write Miss with Write-Through}

\begin{enumerate}
\item Determine it's a write miss
\item Stall CPU
\item Fetch missing block from memory (read operation)
\item Update cache entry with fetched block
\item Write the word to correct position in block
\item Also write to memory
\item Clear stall signal
\item Both levels updated

\subsubsection{Advantages of Write-Through}

\paragraph{Advantage 1: SIMPLICITY}

\begin{itemize}
\item Straightforward to implement
\item No complex consistency protocols
\item Cache controller logic simpler
\item Design principle: Keep cache simple

\paragraph{Advantage 2: CONSISTENCY GUARANTEED}

\begin{itemize}
\item Cache and memory ALWAYS have same values
\item No special handling for discarded blocks
\item Can replace any cache entry anytime
\item Memory always has correct, up-to-date data

\paragraph{Advantage 3: ANSWERS THE OLD BLOCK QUESTION}

\textbf{With write-through policy:}

\begin{itemize}
\item Old block can be safely discarded
\item All updates were written to memory
\item Memory has latest version
\item Future accesses can fetch from memory
\item No data loss

\textbf{Comparison:}

\begin{itemize}
\item Read miss: Old block discarded, data available in memory
\item Write with write-through: Always updated memory, safe to discard

\paragraph{Advantage 4: PARALLEL WRITE AND TAG COMPARE NOW POSSIBLE!}

\textbf{Critical Insight:}
Can now overlap write and tag comparison. Why? Two scenarios:

\textbf{Scenario A: Write Hit}

\begin{itemize}
\item Written to cache, will also write to memory
\item Tag matches, write is correct
\item Both cache and memory updated
\item No problem

\textbf{Scenario B: Write Miss}

\begin{itemize}
\item Written to cache entry (possibly wrong block)
\item Tag mismatch detected
\item Will fetch correct block from memory anyway
\item Will overwrite cache entry with correct block
\item Corrupted data gets replaced immediately
\item Memory has correct version (wasn't corrupted)
\item No lasting damage

\textbf{Result:}

\begin{itemize}
\item Safe to write and tag compare in parallel
\item Saves time (hit latency reduced)
\item Both operations in same clock cycle
\item If hit: Saved time
\item If miss: No harm (will fix cache anyway)

\textbf{Timing Optimization:}

\begin{itemize}
\item Tag comparison time: T_comp
\item Write time: T_write
\item Without overlap: Total = T_comp + T_write
\item With overlap: Total = max(T_comp, T_write)
\item Typically similar delays $\rightarrow$ Nearly 2$\times$ speedup

\subsubsection{Disadvantages of Write-Through}

\paragraph{Disadvantage 1: EXCESSIVE WRITE TRAFFIC}

\begin{itemize}
\item EVERY write goes to memory
\item Memory writes are slow (10-100+ cycles)
\item Generates continuous memory traffic
\item Memory bus congestion

\paragraph{Disadvantage 2: CPU STALLS ON EVERY WRITE}

\textbf{Critical Problem:}

\begin{itemize}
\item Every write requires memory access
\item Memory much slower than cache
\item CPU must stall for EVERY write
\item Wait for memory write to complete

\textbf{Stall Duration:}

\begin{itemize}
\item Memory write: 10-100 CPU clock cycles
\item Every store instruction causes stall
\item Even on write HIT!

\textbf{Example:}

\begin{itemize}
\item Store instruction hits in cache
\item Still must wait for memory write
\item 50 cycle stall for every store
\item Pipeline essentially stops

\textbf{Impact on Programs with Many Writes:}

\begin{itemize}
\item Programs with frequent store instructions
\item Array updates, structure modifications
\item Loop counters being updated
\item String manipulation
\item All suffer severe performance degradation

\textbf{Performance Comparison:}

\begin{itemize}
\item Read hit: < 1 cycle (fast!)
\item Write hit with write-through: 50+ cycles (slow!)
\item Asymmetry: Reads fast, writes catastrophically slow

\textbf{Pipeline Impact:}

\begin{itemize}
\item Recall pipelining lectures: Minimized stalls
\item Worked hard to avoid 1-2 cycle stalls
\item Write-through introduces 50+ cycle stalls regularly
\item Contradicts pipeline optimization goals
\item "Doesn't add up" - unacceptable performance loss

\textbf{Real-World Issue:}

\begin{itemize}
\item Write-through used in some systems
\item But with additional optimizations (write buffers, discussed later)
\item Pure write-through too slow for modern systems

\paragraph{Disadvantage 3: POWER CONSUMPTION}

\begin{itemize}
\item Memory accesses consume power
\item Every write $\rightarrow$ Memory access $\rightarrow$ Power consumption
\item Unnecessary power usage
\item Critical for mobile/embedded systems

\paragraph{Disadvantage 4: MEMORY WEAR}

\begin{itemize}
\item Flash memory: Limited write cycles
\item SSDs wear out with writes
\item Write-through accelerates wear
\item Reduces memory lifespan

\subsection{Resolving the Old Block Question}

\subsubsection{The Question Revisited}

\textbf{Original Question:}

> "What happens to the old block when we fetch a new block from memory on a miss?"

\textbf{Context:}

\begin{itemize}
\item Read or write miss occurs
\item Need to fetch missing block from memory
\item Old block occupies target cache entry
\item Must replace old block with new block
\item Is it safe to discard old block?

\subsubsection{Answer with Write-Through Policy}

\textbf{YES, Safe to Discard}

\paragraph{Reason 1: Memory Has Updated Version}

\begin{itemize}
\item Write-through ensures every write goes to memory
\item All modifications reflected in memory
\item Memory always has latest version of all blocks
\item Old block's latest state is in memory

\paragraph{Reason 2: Can Re-fetch If Needed}

\begin{itemize}
\item Future access to old block's address
\item Will miss in cache (block was replaced)
\item Can fetch from memory again
\item Memory has correct, up-to-date data
\item No data loss

\subsubsection{Example Scenario}

\begin{enumerate}
\item Block A in cache at index 3
\item Block A modified several times
\item Each modification written to cache AND memory
\item Block B (also maps to index 3) is requested
\item Miss occurs for Block B
\item Fetch Block B from memory
\item Replace Block A with Block B at index 3
\item Block A discarded from cache
\item Block A's data safe in memory
\end{enumerate}

10. Later access to Block A: Miss, fetch from memory again

\subsubsection{Comparison with Invalid Entry}

\begin{itemize}
\item If miss due to invalid bit: Obviously safe to replace
\item If miss due to tag mismatch: Safe because of write-through

\subsubsection{Contrast with Future Policy (Teaser)}

\begin{itemize}
\item With other write policies (write-back), answer may differ
\item May NOT be safe to discard old block
\item Will discuss in next lecture

\textbf{Conclusion:}

\begin{itemize}
\item Write-through simplifies replacement
\item No special checks needed before replacing block
\item Always safe to overwrite cache entry
\item Memory serves as reliable backup

\subsection{Parallelism in Write Access with Write-Through}

\subsubsection{The Parallel Write Problem Solved}

\textbf{Original Concern:}

\begin{itemize}
\item Want to overlap write operation and tag comparison
\item Reduce hit latency
\item But risk corrupting data if tag mismatch

\subsubsection{With Write-Through Policy}

\paragraph{Case 1: Write Hit}

\begin{itemize}
\item Write to cache and tag compare happen in parallel
\item Tag matches $\rightarrow$ It was a hit
\item Cache entry correctly updated
\item Also write to memory (per write-through policy)
\item Both cache and memory consistent
\item Time saved: One cycle
\item No problem!

\paragraph{Case 2: Write Miss}

\begin{itemize}
\item Write to cache and tag compare happen in parallel
\item Tag doesn't match $\rightarrow$ It was a miss
\item Cache entry might be corrupted (wrote to wrong block)
\item \textbf{BUT:} About to fetch correct block from memory
\item Will OVERWRITE this cache entry with new block
\item Corrupted data disappears immediately
\item Also, write goes to memory (correct address in memory)
\item End result: Cache fixed, memory correct

\subsubsection{Key Insight}

\begin{itemize}
\item Write-through to memory preserves correctness
\item Memory write goes to CORRECT address (from address bus)
\item Even if cache entry temporarily corrupted
\item Cache entry will be fixed when correct block loaded
\item Memory never corrupted

\subsubsection{Timeline for Write Miss}

Cycle 1: Write to cache (possibly wrong block) + Tag compare
Cycle 1: Also initiate memory write (correct address)
Cycle 2-50: Fetch correct block from memory
Cycle 51: Overwrite cache entry with correct block
Result: Cache correct, memory correct

\subsubsection{Safety Guarantee}

\begin{itemize}
\item \textbf{Memory write:} Targets address from address bus (always correct)
\item \textbf{Cache write:} Targets index (might be for different block)
\item \textbf{If miss:} Cache mistake corrected by fetch
\item \textbf{If hit:} No mistake, everything correct
\item \textbf{In both cases:} End state correct

\subsubsection{Performance Benefit}

\begin{itemize}
\item Saved cycles on write hit path
\item Write and tag compare: Parallel instead of sequential
\item Approximately 2$\times$ faster hit determination
\item Critical for frequent write hits

\subsubsection{Enabled by Write-Through}

\begin{itemize}
\item Only possible because memory updated on every write
\item Other policies may not allow this optimization
\item Write-through sacrifices write performance for simplicity
\item But enables some optimizations

\subsection{Summary of Cache Operations}

\subsubsection{Complete Cache Operation Overview}

\paragraph{READ HIT}

\begin{itemize}
\item Index $\rightarrow$ Tag compare + Valid check $\rightarrow$ Match
\item Extract data block $\rightarrow$ Select word $\rightarrow$ Send to CPU
\item Time: < 1 cycle (hit latency only)
\item No stall
\item Pipeline continues

\paragraph{READ MISS}

\begin{itemize}
\item Index $\rightarrow$ Tag compare + Valid check $\rightarrow$ No match
\item Stall CPU
\item Fetch block from memory (10-100+ cycles)
\item Update cache: Data + Tag + Valid bit
\item Extract word $\rightarrow$ Send to CPU
\item Clear stall
\item Time: Hit latency + Miss penalty
\item Major pipeline disruption

\paragraph{WRITE HIT (with Write-Through)}

\begin{itemize}
\item Index $\rightarrow$ Tag compare + Valid check (parallel with write)
\item Write word to cache block
\item Also write to memory (10-100+ cycles)
\item Stall CPU until memory write completes
\item Time: Hit latency + Memory write time
\item Slower than read hit!

\paragraph{WRITE MISS (with Write-Through)}

\begin{itemize}
\item Index $\rightarrow$ Tag compare + Valid check $\rightarrow$ No match
\item Stall CPU
\item Fetch block from memory
\item Update cache: Data + Tag + Valid bit
\item Write word to cache block
\item Also write to memory
\item Clear stall
\item Time: Hit latency + Miss penalty + Memory write time
\item Even slower than read miss!

\subsubsection{Performance Characteristics}

| Case                                           | Time        | Comment                                                           |
| ---------------------------------------------- | ----------- | ----------------------------------------------------------------- |
| \textbf{Best Case (Read Hit)}                       | < 1 cycle   | Optimal performance. Want this to be most common case             |
| \textbf{Moderate Case (Read Miss)}                  | 50+ cycles  | Acceptable if infrequent. Reason for high hit rate requirement    |
| \textbf{Poor Case (Write Hit with Write-Through)}   | 50+ cycles  | Every write hits this case. Unacceptable for write-heavy programs |
| \textbf{Worst Case (Write Miss with Write-Through)} | 100+ cycles | Rare but extremely slow. Catastrophic when occurs                 |

\textbf{Performance Goal:}

\begin{itemize}
\item Maximize read hits
\item Minimize write impact (better policy needed)
\item Overall hit rate > 99.9%

\subsection{Write-Through Policy Evaluation}

\subsubsection{Summary of Write-Through}

\textbf{Mechanism:}

\begin{itemize}
\item Write to cache (if hit) AND memory
\item Always keep both consistent
\item Memory is authoritative backup

\textbf{Implementation Complexity:}

\begin{itemize}
\item Simple cache controller logic
\item No complex state tracking
\item Straightforward consistency maintenance

\subsubsection{Advantages}

| Advantage                    | Description                                                                                                               |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| \textbf{1. Simplicity}            | Easy to understand, simple to implement, minimal controller complexity, aligns with design principle (simple cache)       |
| \textbf{2. Consistency}           | Cache and memory always consistent, no special synchronization needed, can discard blocks anytime, memory always reliable |
| \textbf{3. Data Safety}           | No data loss on block replacement, memory has all updates, crash recovery simpler, I/O devices see correct data           |
| \textbf{4. Enables Optimizations} | Can overlap write and tag compare, reduces hit latency, safe due to memory backup                                         |

\subsubsection{Disadvantages}

| Disadvantage               | Description                                                                                                                                |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| \textbf{1. Performance Penalty} | Every write stalls CPU, 10-100+ cycle stalls per write, unacceptable for write-intensive programs, contradicts pipeline optimization goals |
| \textbf{2. Memory Traffic}      | Excessive write traffic to memory, memory bus congestion, reduces available bandwidth for read misses, slows down entire system            |
| \textbf{3. Power Consumption}   | Every write powers up memory, unnecessary power usage, battery drain in mobile devices, heat generation                                    |
| \textbf{4. Memory Wear}         | Flash/SSD: Limited write cycles, accelerated wear-out, reduced memory lifespan, particularly bad for SSDs                                  |

\subsubsection{When Write-Through Used}

\paragraph{Suitable Applications}

\begin{itemize}
\item Read-heavy workloads
\item Simple embedded systems
\item Systems requiring guaranteed consistency
\item Safety-critical applications

\paragraph{Real-World Usage}

\begin{itemize}
\item Often combined with write buffers
\item Write buffer: Small queue for pending writes
\item CPU continues after writing to buffer
\item Buffer drains to memory in background
\item Reduces stall impact (will discuss if time permits)

\paragraph{Modern Systems}

\begin{itemize}
\item Pure write-through rarely used alone
\item Too slow for general-purpose computing
\item Alternative: Write-back policy (next lecture)
\item Trade complexity for performance

\subsection{The Need for Alternative Write Policies}

\subsubsection{The Performance Problem}

\paragraph{Write-Heavy Programs}

Many programming patterns involve frequent writes:

\begin{itemize}
\item Array updates in loops
\item Data structure modifications
\item Counter increments
\item Accumulator updates
\item String/buffer operations

\textbf{Example Code:}

\begin{lstlisting}[language=c]
for (int i = 0; i < 1000; i++) {
    array[i] = compute(i);  // Store in every iteration
    sum += array[i];         // Read, accumulate, store
}
\end{verbatim}

\paragraph{With Write-Through}

\begin{itemize}
\item Loop iterations: 1000
\item Stores per iteration: 2 (array[i], sum)
\item Total stores: 2000
\item Cycles per store: 50 (memory write)
\item \textbf{Total stall cycles: 100,000!}
\item Versus computation cycles: Maybe 10,000
\item \textbf{Performance: 10$\times$ slower than necessary!}

\subsubsection{Pipeline Impact}

\begin{itemize}
\item Pipelining designed to execute 1 instruction/cycle (ideal)
\item Write-through: 50 cycles per store instruction
\item Pipeline utilization: ~2% (1/50)
\item Completely defeats pipelining benefits

\subsubsection{Comparison with Read Operations}

| Operation  | Time        | Frequency | Acceptability    |
| ---------- | ----------- | --------- | ---------------- |
| Read hit   | < 1 cycle   | Common    | Fast             |
| Read miss  | 50 cycles   | Rare      | Acceptable       |
| Write hit  | 50 cycles   | Frequent  | \textbf{Unacceptable} |
| Write miss | 100+ cycles | Rare      | Terrible         |

\subsubsection{The Contradiction}

\begin{itemize}
\item Spent lectures optimizing pipeline
\item Minimized hazards, used forwarding, prediction
\item Eliminated 1-2 cycle stalls
\item Now introducing 50+ cycle stalls on every write!
\item "Doesn't add up" - need better solution

\subsubsection{Question Raised}

\textbf{"What can we do to avoid this situation?"}

\textbf{Student Insight:}

> "We can write to memory only when we want to replace that cache block with different data"

\textbf{Instructor Response:}

> "Exactly! That becomes a different write policy."

\subsubsection{Teaser for Next Lecture}

\begin{itemize}
\item Alternative policy: \textbf{Write-Back}
\item Write to cache only, not memory immediately
\item Write to memory only when necessary
\item Much better performance
\item Added complexity in return
\item Will discuss in detail next class

\subsection{Lecture Conclusion}

\subsubsection{Topics Covered}

\paragraph{1. Complete Read Access Process}

\begin{itemize}
\item Index $\rightarrow$ Tag compare $\rightarrow$ Valid check $\rightarrow$ Hit/Miss
\item Parallel data extraction and word selection
\item Hit: Send data immediately
\item Miss: Fetch from memory, stall CPU

\paragraph{2. Read Miss Handling}

Six-step process:

\begin{enumerate}
\item Stall CPU
\item Request block from memory
\item Wait for response
\item Update cache entry (data, tag, valid)
\item Send data to CPU
\item Clear stall

\begin{itemize}
\item Miss penalty: 10-100+ cycles

\paragraph{3. Write Access Process}

\begin{itemize}
\item Similar to read: Index $\rightarrow$ Tag compare $\rightarrow$ Valid check
\item Difference: Must write data to cache
\item Use demultiplexer to direct data to correct word

\paragraph{4. Data Consistency Problem}

\begin{itemize}
\item Writing to cache creates inconsistency
\item Cache has new value, memory has old value
\item Need policy to maintain consistency

\paragraph{5. Write-Through Policy}

\begin{itemize}
\item Write to both cache and memory on every write
\item Advantages: Simple, consistent, safe
\item Disadvantages: Slow, excessive traffic, poor performance

\paragraph{6. Old Block Question Resolved}

\begin{itemize}
\item With write-through: Safe to discard
\item Memory has updated version
\item Can re-fetch if needed later

\paragraph{7. Parallel Write Optimization}

\begin{itemize}
\item Can overlap write and tag compare
\item Write-through makes this safe
\item Reduces hit latency

\paragraph{8. Performance Issues}

\begin{itemize}
\item Write-through too slow for write-intensive programs
\item Every write causes long stall
\item Need better policy

\subsubsection{Next Lecture Preview}

\textbf{Topics to Cover:}

\begin{itemize}
\item Write-Back policy (delayed writes)
\item Dirty bit concept
\item When to write back to memory
\item Performance improvements
\item Complexity tradeoffs
\item Block replacement with write-back
\item Comparison: Write-through vs. Write-back
\item Real-world cache designs

\textbf{Implementation Details:}

\begin{itemize}
\item Write buffer optimization for write-through
\item Handling dirty blocks on replacement
\item Write-back state machine
\item Performance analysis

\textbf{Advanced Topics (if time):}

\begin{itemize}
\item Write-allocate vs. no-write-allocate
\item Write-combining
\item Victim caches
\item Multi-level caches with different policies

\textbf{The Goal:}

\begin{itemize}
\item Understand tradeoffs between simplicity and performance
\item Choose appropriate policy for application
\item Design efficient cache systems

\textbf{Key Insight:}
Write-through sacrifices performance for simplicity. In modern systems, performance is critical, so more complex policies are necessary despite added complexity.

\subsection{Key Takeaways}

\begin{enumerate}
\item \textbf{Cache read hit} completes in single cycle—tag match and valid bit set indicate data available immediately from cache.

\begin{enumerate}
\item \textbf{Cache read miss} requires multiple cycles—must fetch entire block from main memory, update cache entry, set valid bit, then retry access.

\begin{enumerate}
\item \textbf{Cache controller} implements state machine—managing transitions between idle, compare tags, fetch block, and write cache states.

\begin{enumerate}
\item \textbf{Tag comparison} determines hit/miss—stored tag must match address tag AND valid bit must be set for successful hit.

\begin{enumerate}
\item \textbf{Block fetch} retrieves entire block from memory—exploiting spatial locality by bringing multiple words that will likely be accessed soon.

\begin{enumerate}
\item \textbf{Valid bit initialization} crucial at startup—all valid bits cleared to zero, preventing false hits on random cache data.

\begin{enumerate}
\item \textbf{Write operations} complicate cache design—must maintain consistency between cache and main memory through careful policy choices.

\begin{enumerate}
\item \textbf{Write-through policy} updates both cache and memory on every write—simple consistency but severe performance penalty.

\begin{enumerate}
\item \textbf{Write-through advantages}: Simple implementation, main memory always current, no dirty bit needed, straightforward crash recovery.

10. \textbf{Write-through disadvantages}: Every write causes slow memory access (~100 ns), dramatically reduces performance, wastes memory bandwidth.

11. \textbf{Write buffers} partially mitigate write-through penalty—CPU writes to buffer and continues, buffer writes to memory asynchronously.

12. \textbf{Write buffer depth} typically 4-8 entries—balances performance improvement against hardware cost and complexity.

13. \textbf{Write buffer full} forces CPU stall—occurs during write-intensive code sections, limiting write-through effectiveness.

14. \textbf{Write miss policies} determine cache behavior—write-allocate (fetch block first) versus no-write-allocate (write directly to memory).

15. \textbf{Write-allocate} exploits temporal locality—if just written location likely accessed again soon, fetching to cache improves future performance.

16. \textbf{No-write-allocate} avoids fetch overhead—appropriate when written locations unlikely to be accessed soon.

17. \textbf{Policy combinations} affect overall performance—write-through typically paired with no-write-allocate for consistency.

18. \textbf{Cache consistency} means cache and memory agree on data values—critical correctness requirement across all cache operations.

19. \textbf{Performance impact} of write policies substantial—write-through can increase memory traffic by 15-20% in typical programs.

20. \textbf{Write-back policy} introduced as superior alternative—defers memory writes until block eviction, dramatically reducing memory traffic.

\subsection{Summary}

Detailed examination of cache memory operations reveals the sophisticated control logic required to manage read and write accesses while maintaining data consistency between cache and main memory. Read operations follow straightforward paths: hits deliver data in single cycle via tag comparison confirming both tag match and valid bit set, while misses trigger multi-cycle sequences fetching entire blocks from main memory, updating cache entries, setting valid bits, and retrying accesses. The cache controller implements these sequences through state machine logic managing transitions between idle, tag comparison, block fetching, and cache writing states. Write operations introduce significant complexity and performance implications through policy choices determining how cache and memory stay synchronized. Write-through policy, updating both cache and memory on every write, offers simplicity and guaranteed consistency—main memory always reflects current data state, enabling straightforward crash recovery and multi-processor coherence. However, write-through's performance penalty proves severe: every write operation incurs ~100 nanosecond memory access delay, effectively eliminating cache benefit for write-heavy code sections and wasting substantial memory bandwidth on updates. Write buffers provide partial mitigation by decoupling CPU from memory write delays, allowing processors to write to small hardware queues and continue execution while buffer contents asynchronously propagate to main memory. Typical write buffers holding 4-8 entries balance performance improvement against hardware cost, though write-intensive code can still fill buffers and force CPU stalls. Write miss policies—write-allocate (fetch block before writing) versus no-write-allocate (write directly to memory)—represent additional design choices affecting performance based on program access patterns. Write-allocate exploits temporal locality, benefiting code that writes then soon reads same locations, while no-write-allocate avoids fetch overhead for write-once scenarios. Write-through typically pairs with no-write-allocate for policy consistency. The fundamental limitation—that write-through forces memory access on every write regardless of whether data will be accessed again—motivates write-back policies introduced in subsequent lectures, which defer memory writes until block eviction and thereby dramatically reduce memory traffic. Understanding these operational details and policy tradeoffs proves essential for appreciating how real cache implementations balance performance, complexity, consistency, and correctness requirements in practical computer systems.
