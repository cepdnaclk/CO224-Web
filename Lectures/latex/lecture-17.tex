\section{Lecture 17: Cache Hierarchies and Real Implementations}

\emph{By Dr. Isuru Nawinne}

\subsection{Introduction}

This lecture explores cache hierarchies in modern computer systems, examining how multiple levels of cache work together to optimize memory access performance through careful balance of hit latency versus hit rate. We analyze real-world implementations including Intel's Skylake architecture, understanding the design decisions behind multi-level cache organizations where L1 caches prioritize speed, L2 caches balance capacity and latency, and L3 caches provide large shared storage across processor cores. The examination of associativity tradeoffs—from direct-mapped through set-associative to fully associative designs—reveals how hardware complexity, power consumption, and performance interact in practical cache systems.

\subsection{Recap: Associativity Comparison Results}

From the previous lecture's example using a 4-block cache with three different organizations:

\subsubsection{Direct Mapped Cache}

\begin{itemize}
\item \textbf{Result}: 5 misses, 0 hits
\item \textbf{Cold misses}: 3 (compulsory, unavoidable)
\item \textbf{Conflict misses}: 2 (data evicted then accessed again)
\item \textbf{Utilization}: Poor - only 2 of 4 slots used
\item \textbf{Hit rate}: 0% in this example

\subsubsection{2-Way Set Associative Cache}

\begin{itemize}
\item \textbf{Result}: 4 misses, 1 hit
\item \textbf{Cold misses}: 3
\item \textbf{Conflict misses}: 1
\item \textbf{Utilization}: 2 of 4 slots used
\item \textbf{Hit rate}: 20% - better than direct mapped

\subsubsection{Fully Associative Cache (4-way)}

\begin{itemize}
\item \textbf{Result}: 3 misses, 2 hits
\item \textbf{Cold misses}: 3 (only unavoidable misses)
\item \textbf{Conflict misses}: 0
\item \textbf{Utilization}: Best - 3 of 4 slots used
\item \textbf{Hit rate}: 40% - best performance

\subsubsection{Key Observations}

\begin{itemize}
\item Higher associativity $\rightarrow$ better hit rate
\item Higher associativity $\rightarrow$ reduced conflict misses
\item Cold misses occur at program start and when new addresses are accessed
\item System reaches "steady state" with mostly conflict misses after initial cold misses
\item Performance improvement comes at cost of complexity and power

\subsection{Cache Configuration Parameters}

\subsubsection{Primary Parameters}

\paragraph{1. Block Size}

\begin{itemize}
\item Size of a single block in bytes
\item Cache deals with memory in blocks
\item CPU deals with cache in words/bytes

\paragraph{2. Set Size}

\begin{itemize}
\item Number of sets in the cache
\item Direct mapped: number of sets = number of entries
\item Fully associative: only 1 set
\item Can be confusing - refers to number of sets, not size of each set

\paragraph{3. Associativity}

\begin{itemize}
\item Number of ways in a set
\item Number of blocks that can be stored in one set
\item 1-way = direct mapped
\item 2-way = two-way set associative
\item N-way = N blocks per set

\subsubsection{Cache Size Calculation}

Total Cache Size = Block Size $\times$ Set Size $\times$ Associativity

\subsubsection{Secondary Parameters}

\paragraph{4. Replacement Policy}

\begin{itemize}
\item LRU (Least Recently Used)
\item Pseudo-LRU (PLRU)
\item FIFO (First In First Out)
\item Others

\paragraph{5. Write Policy}

\begin{itemize}
\item Write-through
\item Write-back

\paragraph{6. Other Optimization Techniques}

\begin{itemize}
\item Prefetching mechanisms
\item Write buffer size
\item Communication protocols

\subsubsection{Configuration Definition}

\begin{itemize}
\item Fixing values for all these parameters defines a specific cache configuration
\item Performance and power consumption are determined by configuration
\item External factors: memory access patterns from CPU/program

\subsection{Improving Cache Performance}

\subsubsection{Average Access Time Equation}

T_avg = Hit Latency + Miss Rate $\times$ Miss Penalty

Three main factors can be optimized as below.

\subsection{Hit Rate Improvement}

\subsubsection{Method 1: Increase Cache Size}

\textbf{Approach}:

\begin{itemize}
\item Most obvious and intuitive method
\item More slots $\rightarrow$ can hold more data $\rightarrow$ more likely to get hits

\textbf{Limitations}:

\begin{itemize}
\item Very expensive (SRAM costs ~$2000/GB)
\item SRAM uses cutting-edge technology, same as CPU
\item Must be fast enough to work at CPU speed
\item Usually located inside CPU core
\item Practical limit on how much cache can be added

\subsubsection{Method 2: Increase Associativity}

\textbf{Benefits}:

\begin{itemize}
\item Higher associativity $\rightarrow$ better hit rate
\item Reduces conflict misses
\item Most popular technique for given cache size

\textbf{Trade-offs}:

\begin{itemize}
\item Increases hit latency
\item Increases power consumption
\item Increases hardware cost

\subsubsection{Method 3: Cache Prefetching}

\textbf{Concept}:

\begin{itemize}
\item Fetch data before it's needed
\item Similar to branch prediction in CPU
\item Reduces cold misses (compulsory misses)
\item Can also reduce conflict misses

\textbf{Types of Prefetching}:

\begin{itemize}
\item Software prefetching (compiler-based)
\item Hardware prefetching
\item Hybrid software-hardware approaches

\textbf{Benefits}:

\begin{itemize}
\item Can predict and fetch data before CPU requests it
\item Reduces effective miss rate
\item Can significantly improve performance for predictable access patterns

\textbf{Limitations}:

\begin{itemize}
\item Not 100% accurate
\item Wrong predictions waste power and bandwidth
\item Requires additional hardware
\item Increases complexity

\subsection{Hit Latency Optimization}

\subsubsection{Relationship with Hit Rate}

\textbf{Fundamental Trade-off}:

\begin{itemize}
\item Hit rate and hit latency are tied together
\item Improving hit rate often increases hit latency
\item Improving hit latency often reduces hit rate
\item Need to find optimal balance

\textbf{Examples}:

\begin{itemize}
\item Higher associativity $\rightarrow$ better hit rate BUT higher hit latency
\item Smaller, simpler cache $\rightarrow$ lower hit latency BUT worse hit rate

\textbf{Design Challenge}:

\begin{itemize}
\item Must balance these competing factors
\item Depends on application requirements
\item Different trade-offs for different use cases

\subsection{Miss Penalty Improvement}

\subsubsection{Miss Penalty Definition}

\begin{itemize}
\item Time spent servicing a cache miss
\item Time to fetch missing block from memory

\subsubsection{Method 1: Optimize Communication}

\begin{itemize}
\item Improve bus technology between cache and memory
\item Increase bus width
\item Increase bus speed
\item Optimize bus arbitration
\item Better communication protocols
\item This assumes best possible communication is already in place

\subsubsection{Method 2: Cache Hierarchy (Main Focus)}

\begin{itemize}
\item Use multiple levels of cache
\item Each level optimized differently
\item Most effective technique for reducing miss penalty

\subsection{Cache Hierarchy (Multi-Level Caches)}

\subsubsection{Concept}

Instead of a single cache between CPU and memory, use multiple cache levels: L1, L2, L3, etc., with each level serving as backup for the level above.

\subsubsection{Terminology}

\begin{itemize}
\item \textbf{L1 (Level 1)}: Top-level cache, closest to CPU
\item \textbf{L2 (Level 2)}: Second-level cache
\item \textbf{L3 (Level 3)}: Third-level cache (in some systems)
\item \textbf{Top-level cache}: Fastest, smallest
\item \textbf{Last-level cache}: Slowest (but still fast), largest

\subsubsection{Operation}

\begin{enumerate}
\item CPU requests data from L1
\item L1 miss $\rightarrow$ request goes to L2 (not directly to memory)
\item L2 miss $\rightarrow$ request goes to L3 (if exists)
\item Last-level miss $\rightarrow$ request goes to main memory

\subsubsection{Benefits}

\begin{itemize}
\item Reduced effective miss penalty for L1
\item Most L1 misses served by L2 in few cycles (2-4 cycles)
\item Only L2 misses incur full memory penalty (100+ cycles)
\item Overall average miss penalty greatly reduced

\subsubsection{Effective Miss Penalty}

For L1 cache:

Effective Miss Penalty = L2 Hit Latency + L2 Miss Rate $\times$ L2 Miss Penalty

If L2 has good hit rate:

\begin{itemize}
\item L2 miss rate is low
\item Most L1 misses served quickly by L2
\item Effective penalty much less than going to memory

\subsubsection{Example Calculation}

Given:

\begin{itemize}
\item L1 miss rate: 5%
\item L2 hit rate: 99.9%
\item L2 hit latency: 3 cycles
\item Memory penalty: 100 cycles

L1 effective penalty = 3 + 0.001 $\times$ 100 = 3.1 cycles

\subsection{Optimization Strategies for Multi-Level Caches}

\subsubsection{Why Not One Big Cache?}

\begin{itemize}
\item Different levels can be optimized for different goals
\item Splitting allows specialized optimization
\item Better overall performance than single large cache

\subsection{L1 Cache Optimization - Optimize for Hit Latency}

\subsubsection{Goal}

Minimize hit latency

\subsubsection{Rationale}

\begin{itemize}
\item Critical for CPU clock cycle time
\item Memory access is slowest pipeline stage
\item Determines overall CPU clock period
\item Lower L1 hit latency $\rightarrow$ shorter clock cycle $\rightarrow$ higher CPU frequency

\subsubsection{Characteristics}

\begin{itemize}
\item Small size
\item Lower associativity (2-way, 4-way, sometimes 8-way)
\item Fast response time
\item Accept moderate hit rate (e.g., 95%)

\subsubsection{Trade-off}

\begin{itemize}
\item Sacrifice some hit rate for speed
\item Slightly higher miss rate acceptable
\item Misses handled by L2

\subsection{L2 Cache Optimization - Optimize for Hit Rate}

\subsubsection{Goal}

Maximize hit rate

\subsubsection{Rationale}

\begin{itemize}
\item Serve most L1 misses
\item Minimize accesses to main memory
\item Reduce effective L1 miss penalty

\subsubsection{Characteristics}

\begin{itemize}
\item Larger size
\item Higher associativity (8-way, 16-way, or even fully associative)
\item Very high hit rate (99.9% or better)
\item Can tolerate higher hit latency

\subsubsection{Trade-off}

\begin{itemize}
\item Higher latency acceptable
\item Not on critical path for most accesses
\item Priority is catching L1 misses

\subsection{Associativity Comparison}

\textbf{Question}: Which level has higher associativity?

\textbf{Answer}: L2 (and L3 if present) have higher associativity

\subsubsection{Reasoning}

\begin{itemize}
\item L2 optimized for hit rate
\item Higher associativity $\rightarrow$ better hit rate
\item L1 optimized for latency
\item Lower associativity $\rightarrow$ faster access

\subsubsection{Combined Effect}

\begin{itemize}
\item \textbf{L1}: Fast but moderate hit rate (e.g., 95-98%)
\item \textbf{L2}: Slower but excellent hit rate (e.g., 99-99.9%)
\item \textbf{Most accesses}: L1 hit (fast path)
\item \textbf{Most L1 misses}: L2 hit (medium path, few cycles)
\item \textbf{Very few accesses}: Main memory (slow path, 100+ cycles)

\textbf{Overall result}: Much better average performance

\subsection{Physical Implementation of Cache Hierarchy}

\subsubsection{L1 Cache}

\begin{itemize}
\item Almost always on-chip (inside CPU die)
\item Integrated within CPU core
\item Smallest but fastest
\item Typically split into:
\item L1 instruction cache (L1-I)
\item L1 data cache (L1-D)

\subsubsection{L2 Cache}

\begin{itemize}
\item Usually on-chip (same die as CPU)
\item Can be off-chip in some designs
\item Larger than L1
\item May be unified (instruction + data) or split
\item If multi-core: may be per-core or shared

\subsubsection{L3 Cache}

\begin{itemize}
\item Common in multi-processor/multi-core systems
\item Usually on-chip in modern designs
\item Can be off-chip in some architectures
\item Typically unified and shared among all cores
\item Largest cache level

\subsubsection{Design Variations}

Different implementations based on:

\begin{itemize}
\item Performance requirements
\item Power budget
\item Cost constraints
\item Target application
\item Number of cores

\subsection{Real World Example: Intel Skylake Architecture}

\textbf{Source}: wikichip.org

\subsubsection{Architecture Overview}

\begin{itemize}
\item Mainstream Intel architecture from ~2015
\item Used in Core i3, i5, i7 processors
\item Standard desktop/PC processors

\subsubsection{Dual-Core Layout Analysis}

\paragraph{Execution Units}

\begin{itemize}
\item Two separate processor cores visible
\item Integer ALUs (arithmetic logic units)
\item Floating-point units
\item Multipliers, dividers
\item Other arithmetic hardware

\paragraph{Pipeline Support Hardware}

\begin{itemize}
\item Takes up as much space as execution units
\item Out-of-order scheduling logic
\item Branch prediction units
\item Multiple issue hardware
\item Decoding logic
\item Control logic

\subsubsection{Cache Implementation}

\paragraph{L1 Data Cache}

\begin{itemize}
\item Separate for each core
\item Located close to execution units and memory management
\item \textbf{8-way set associative}
\item Smaller size (32KB typical)
\item Close to where addresses are generated

\paragraph{L1 Instruction Cache}

\begin{itemize}
\item Separate for each core
\item Located close to instruction fetch and decode units
\item Near out-of-order scheduling hardware
\item \textbf{8-way set associative}
\item Smaller size (32KB typical)

\paragraph{L2 Cache}

\begin{itemize}
\item Shared between instruction and data
\item Larger than L1 (256KB in this example)
\item \textbf{4-way set associative} (in this design)
\item Located between L1 and memory
\item Serves both L1-I and L1-D misses

\subsubsection{Memory Hierarchy}

\begin{itemize}
\item Separate buffers for load and store instructions
\item Buffers before and after cache
\item Memory management unit
\item Connection to L3 cache (if present) via bus

\subsubsection{Design Observations}

\begin{itemize}
\item Physical placement matches logical function
\item Data cache near execution units
\item Instruction cache near fetch/decode
\item Shared L2 in middle position
\item Significant die area for cache
\item Even more area for pipeline optimization

\subsubsection{Why Higher L1 Associativity Here?}

\begin{itemize}
\item 8-way seems high for L1
\item But size is small (32KB)
\item Other pipeline stages may be bottleneck
\item Clock period limited by other factors
\item Can afford higher associativity without hurting cycle time
\item Depends on overall CPU design

\subsubsection{Multi-Core Configuration}

\begin{itemize}
\item Each core has own L1-I and L1-D
\item Each core has own L2
\item All cores share L3
\item L3 connects via bus system

\subsubsection{Additional Features}

\begin{itemize}
\item Physical register files (integer and vector)
\item Store/load buffers
\item Pre-decoding hardware
\item Complex x86 instruction handling
\item Many optimizations for real-world performance

\subsection{Recommendations for Further Study}

\subsubsection{Resource: wikichip.org}

\textbf{Content Available}:

\begin{itemize}
\item Detailed CPU architecture information
\item Real implementation details
\item Various processor families:
\item Intel x86 architectures
\item ARM implementations
\item AMD processors
\item Other architectures

\textbf{Benefits}:

\begin{itemize}
\item See concepts in real hardware
\item Understand practical trade-offs
\item Compare different design approaches
\item Learn industry practices

\subsection{Key Takeaways}

\begin{enumerate}
\item Cache hierarchies reduce effective miss penalty
\item Different levels optimized for different goals:
\item L1: Hit latency (speed)
\item L2/L3: Hit rate (coverage)
\item Multi-level caches balance competing requirements
\item Real implementations show concepts in practice
\item Design decisions depend on:
\item Performance targets
\item Power budget
\item Cost constraints
\item Application requirements
\item Modern CPUs use sophisticated cache hierarchies
\item Cache takes significant portion of CPU die area
\item Pipeline optimizations also require substantial hardware

\subsection{Summary}

Cache hierarchies represent one of the most effective techniques for improving memory system performance. By using multiple levels of cache, each optimized for different objectives, modern processors achieve both low latency and high hit rates. The L1 cache prioritizes speed to minimize clock cycle time, while L2 and L3 caches prioritize capacity and hit rate to reduce memory access frequency. Real-world implementations, such as Intel's Skylake architecture, demonstrate these principles in practice, showing how careful cache design enables high-performance computing while managing the constraints of power, cost, and chip area.
